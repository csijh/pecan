<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb" xml:lang="en-gb">
<head>
<title>Pecan Manual</title>
<style>pre { margin-left:40px; }</style>
</head>
<body>

<img src="pecan.png" width="300" height="65" />
<hr/>

<!--
TODO: specifying an entry point, allowing any fixed number of outputs. 
// Advantage of explicit call stack instead of actual calls:
//   leaves open techniques such as examining the stack on failure
//   to help with error reporting, and leaving the parse state intact to be
//   restarted later.
// Efficiency difficult to estimate: interpretive loop overhead (as with
//   table-driven LR parsers!) but some optimizations such as
//   tail calls are easier, and memory compactness is important.
-->

<h1>Pecan Reference Manual</h1>

<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#install">Installation</a></li>
<li><a href="#notation">Notation</a>
<ul>
<li><a href="#rules">Rules</a></li>
<li><a href="#comments">Comments</a></li>
<li><a href="#parentheses">Parentheses</a></li>
<li><a href="#continuations">Continuations</a></li>
<li><a href="#choices">Choices</a></li>
<li><a href="#sequences">Sequences</a></li>
<li><a href="#repetition">Repetition</a></li>
<li><a href="#lookahead">Lookahead</a></li>
<li><a href="#characters">Characters</a></li>
<li><a href="#tags">Tags</a></li>
<li><a href="#actions">Actions</a></li>
<li><a href="#errors">Errors</a></li>
</ul>
</li>
<li><a href="#testing">Testing</a></li>
<li><a href="#checks">Consistency Checks</a>
<ul>
<li><a href="#type">Type Checking</a></li>
<li><a href="#loop">Loop Checking</a></li>
<li><a href="#token">Token Checking</a></li>
<li><a href="#output">Output Checking</a></li>
</ul>
<li><a href="#java">Generating Code</a>
<ul>
<li><a href="#code">Bytecode</a></li>
</ul>
</li>
<li><a href="#side">Side Effects</a></li>
</li>
<li><a href="#transforms">Transforms</a></li>
<li><a href="#pecan">The Pecan grammar</a></li>
</ul>

<h2 id="intro">Introduction</h2>

<p>Pecan is a tool for developing and checking grammars, and generating scanners
and parsers.  It is aimed at programming languages, <a
href="https://en.wikipedia.org/wiki/Domain-specific_language">domain specific
languages</a>, and other unambiguous languages.  It provides a precise grammar
notation, which is independent of implementation language, based on recursive
descent with lookahead. This manual describes Pecan version 1.0 which has these
features:</p>

<ul>

<li>a grammar is an executable prototype parser</li>

<li>output actions and error markers are included</li>

<li>transformations preserve actions and error handling</li>

<li>many consistency checks are applied</li>

<li>there is explicit support for development and testing</li>

<li>scanner and parser generation is via a bytecode</li>

<li>an interpreter for the bytecode can be written in any language</li>

</ul>

<p>Other grammar languages are usually based on
the <a href="http://en.wikipedia.org/wiki/Context-free_grammar">context free
grammar</a> (CFG) formalism,
the <a href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">parsing
expression grammar</a> (PEG) notation, or
the <a href="https://en.wikipedia.org/wiki/Recursive_descent_parser">recursive
descent</a> (RD) approach.</p>

<p>The previous version of Pecan was 0.4. Changes are:</p>

<ul>
<li>actions are now allowed at the start of a left hand choice</li>
<li>actions are delayed in case they need to be discarded</li>
<li>error markers are no longer postfix</li>
<li>string tags use double quotes instead of backquotes</li>
<li>string tags have definitions to give them names</li>
<li>parser generation is via bytecode</li>
</ul>

<h3>CFG grammars</h3>

<p>The CFG notation (in the form of BNF and many variations) has been the main
formalism for grammars for a long time, and it has been well studied.  However,
it can be, and has been, argued quite strongly that it is not an ideal formalism
to use for unambiguous languages.  According to <a
href="http://dl.acm.org/citation.cfm?id=964001.964011">Bryan Ford</a>:</p>

<blockquote>The power of generative grammars to express ambiguity is crucial to
their original purpose of modelling natural languages, but this very power makes
it unnecessarily difficult both to express and to parse machine-oriented
languages using CFGs. </blockquote>

<p>The most fundamental theoretical problem is that grammars can contain
ambiguities, and there is no computable algorithm to detect whether or not a
grammar is unambiguous. There is also the associated practical problem that many
published CFG grammars contain ambiguities.  Some of these are local, i.e. a
particular rule allows two possible parses, but there is a surrounding more
global rule that resolves the ambiguity by only accepting one of them.  But many
are inherent, i.e. the ambiguity is resolved only by accompanying descriptive
text.</p>

<p>There is also the problem that a CFG grammar describes a language in a
generative way, which is not directly and intuitively linked to the recognition
problem faced in parsing.</p>

<p>Conventional CFG-based parser generators have a variety of other flaws,
partly due to the difficulties of the CFG formalism, and partly due to the age
of their designs.  They often have arbitrary heuristic rules for disambiguation.
They often support a subset of CFG grammars which is not at all intuitive. They
often use bottom-up parsing techniques to make parsers near-linear, and this
makes the way they operate impenetrable. And they often support actions and
error reporting using embedded code, which is an extremely non-modular way for
different portions of code to interact.</p>

<p>These problems with the CFG formalism might be worth putting up with, if the
formalism had excellent theoretical or practical properties.  But it doesn't.
For example, CFG grammars have poor composability, the set of CFG grammars is
not closed under intersection or difference and, in general, parsing CFG
grammars takes O(n<sup>3</sup>) time in the worst case.</p>

<h3>PEG grammars</h3>

<p>The main difference with PEG grammars is that the symmetrical choice operator
<code>|</code> is replaced by an ordered choice operator <code>/</code> which
has a more operational meaning.  The expressive power of PEG grammars is
extremely close to that of CFG grammars.  Examples such as palindromes which are
a problem for PEG grammars are typically also a problem for the efficient LR or
LALR subsets of CFG grammars.</p>

<p>The theoretical and practical properties of the PEG formalism are superior to
the CFG formalism in almost every measurable way.  PEG grammars never contain
any ambiguity, they have a direct operational meaning as parsers, they are
composable, they are closed under intersection and difference, and they can be
parsed in linear time using the packrat algorithm.  This makes the PEG grammar
formalism a much better starting point for studying grammars and parsers.</p>

<p>There is a minor problem with PEG grammars, which is that they don't support
left recursion.  There have been attempts to add it, but the results are not
compelling.  It is worth noting, though, that left recursion is arguably
unintuitive, that it would probably never have become common if it weren't for
the prevalence of CFG grammars, and that it is well known how to transform it
away. Most uses of left recursion are very simple, of the kind <code>a = x | a
y</code> which translates into <code>a = x y*</code>.</p>

<p>There are more serious practical problems with PEG parsers. Although the
packrat algorithm is linear, it is slower and more space hungry than one would
like for efficient large scale parsing.  Also, because if its relatively
uncontrolled backtracking, it is difficult to produce accurate error messages,
and difficult to express actions.  And published PEG grammars can often be
rather unreadable, in the same way that regular expressions tend to be. As a
result, PEG grammars are usually only used for search expressions or for small
domain specific languages.</p>

<h3>Recursive Descent Grammars</h3>

<p>Recursive descent parsers have the advantage of being efficient, intuitive,
and capable of being hand-written.  They can also be embedded in programming
languages, e.g. in the form of <a
href="http://en.wikipedia.org/wiki/Parser_combinator">parser
combinators</a>.</p>

<p>Perhaps the main problem with recursive descent is that a simple approach is
hardly ever enough.  In order to deal with the difficult cases which arise in
practice, there have to be some extra features such as lookahead.  These extra
features are often added in an ad hoc fashion on practical grounds.  That often
leads to a situation where it is difficult to tell exactly what range of
grammars or languages are supported by any given recursive descent system.</p>

<p>Embedding a parsing system into a language, e.g. in the form of combinators,
can be very convenient, especially for small domain specific languages where the
grammar is known in advance.  A system like that has a natural feel for
programmers, often with good control over backtracking.  However, a combinator
library is typically limited to one programming language. If the grammar needs
to be developed from scratch, or translated from a different formalism (which is
common because of the prevalence of CFG grammars), or checked for consistency,
or optimized to produce a fast parser, then there is a case for a separate
formalism such as Pecan.</p>

<h3>Pecan grammars</h3>

<p>Pecan uses a language which is based on the PEG notation.  However, the
semantics is that of simple recursive descent, with explicitly controlled
lookahead and backtracking.</p>

<p>The language is independent of any host programming language.  Nevertheless,
grammars can express actions and error reporting, and can be executed
symbolically for development and testing.  Once developed, a grammar can be
converted into a parser in any desired language, either by hand or
automatically.</p>

<p>The Pecan system explicitly provides support for the process of developing
grammars, in the form of consistency checks on grammars, features for automated
testing, and a context in which manual transformations can be carried out.
Having a separate notation for grammars, rather than writing a parser by hand,
helps to focus attention on the various issues that have to be addressed,
separately from the programming details, but that also makes grammars rather
dense.  If a grammar is not given in advance, creating it or translating it
accurately can be very difficult.  In addition, programmers typically use parser
generators only rarely, and are often not grammar experts.  As a result, support
for test-driven grammar development is an important feature of Pecan.</p>

<h2 id="install">Installation</h2>

<p>Pecan is a tool for developing grammars and generating parsers in a variety
of programming languages.  The Pecan system is written in Java, so the first
step is to make sure a reasonably up-to-date version of Java is properly
installed.  This can be tested by typing:</p>

<pre>javac -version
java -version
</pre>

<p>Pecan is provided as an executable jar file <code>pecan.jar</code>.  The jar
file contains both the compiled program and also the source code.  If
this jar file is downloaded, it can be run with:</p>

<pre>java -jar pecan.jar ...
</pre>

<p>However, it is more convenient to create a batch file or shell script, or use
an alias or equivalent, so that it can be run just by typing:</p>

<pre>pecan ...
</pre>

<p>The way to do this differs from system to system.</p>

<h2 id="notation">Notation</h2>

<p>The Pecan language is a gramamr language for writing parsers, closely based
on the <a href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">PEG</a>
notation. Pecan uses the sequential choice operator <code>x/y</code> but, unlike
the PEG notation, the operator does not automatically involve backtracking.
There are separate lookahead operators, for the controlled use of backtracking,
which provide greater control over the progress of parsing.  It is much easier
to generate accurate error messages, and parsers can be kept reasonably
efficient by the programmer, without the space overhead of such techniques as
the PEG packrat algorithm.</p>

<p>As with PEG grammars, any parsing expression within a grammar represents a
matching operation which succeeds or fails when applied at a particular position
in the input, and which may cause progress, i.e. may cause the current position
to move forwards in the input, according to how much of it is matched.  Unlike
PEG grammars, there is only a single possible outcome for this operation.</p>

<h3 id="rules">Rules</h3>

<p>A parser consists of a sequence of rules, each of which gives a name to a
parsing expression.  The first rule specifies the output of the whole parsing
process.  For example, a parser might start with:</p>

<pre>module = function+ end

function = id arguments body

end = Uc!
...
</pre>

<p>Apart from being the starting point of the parser, the first rule is not
treated in any special way.  In particular, it may succeed without consuming
all the input.  So an explicit test is often included, as here, to check that
the end of the input has been reached.</p>

<p>The text of a parser is assumed to be encoded using UTF-8, so Unicode
characters can be included.  A rule name starts with any Unicode letter and
continues with Unicode letters or decimal digits or underscores or hyphens.</p>

<p>If a parser fails, only the first failure is reported. There is no attempt to
recover.  If error recovery is required, this can be accomplished by writing a
separate rule to be tried on the remaining input.</p>

<h3 id="comments">Comments</h3>

<p>Comments start with <code>//</code> and extend to the end of the line.  For
example:</p>

<pre>// A number is a sequence of one or more digits.
number = '0123456789'+
</pre>

<p>There is no multi-line comment convention.</p>

<h3 id="parentheses">Parentheses</h3>

<p>Parentheses, i.e. round brackets, have their usual meaning, indicating
grouping of operations.  For example:</p>

<pre>(x / y) z
</pre>

<p>means parse <code>x</code> or <code>y</code>, then parse <code>z</code>, as
opposed to:</p>

<pre>x / y z
</pre>

<p>which means either parse <code>x</code>, or parse <code>y</code> and
<code>z</code>.</p>

<h3 id="continuations">Continuations</h3>

<p>No explicit symbol is used to terminate a rule. A rule is continued on the
next line if the last token on the current line is an infix symbol or open
bracket, i.e. one of <code>=/([</code>.  Here is an example rule where each line
except the last ends with <code>=</code> or the <code>/</code> operator:</p>

<pre>atom =
  id /
  number /
  bracket
</pre>

<p>Also, a rule is continued on the next line if the first token of the next
line is an infix symbol or close bracket, i.e. one of <code>=/)]</code>. This
allows an alternative style for a multi-line rule:</p>

<pre>atom
= id
/ number
/ bracket
</pre>

<p>In this example, each line that starts with <code>=</code> or <code>/</code>
is a continuation of the previous line.</p>

<h3 id="choices">Choices</h3>

<p>The choice operator <code>/</code> separates alternatives, which are tried
one after the other.  For example:</p>

<pre>atom = id / number / bracket
</pre>

<p>If parsing of an alternative succeeds, then the parsing of the whole
expression succeeds. Otherwise, if no progress was made, the next alternative is
tried.</p>

<p>The choice operator does not directly involve backtracking.  If any progress
is made while trying an alternative, then the parser is committed to that
alternative, and further alternatives are not tried.  Progress is made on an
alternative if at least one input character or token is matched.</p>

<p>There are separate lookahead operators which allow speculative parsing of an
alternative.</p>

<h3 id="sequences">Sequences</h3>

<p>When items follow each other with no visible operator in between, this
indicates "followed by".  For example:</p>

<pre>assignment = identifier "=" expression
</pre>

<p>This means that an assignment is an identifier followed by an equals sign
followed by an expression.  The items are parsed one by one and, if parsing of
any item fails, parsing of the sequence is abandoned.  Sequencing binds tighter
than the choice operator.</p>

<h3 id="repetition">Repetition</h3>

<p>The three postfix repetition operators are <code>*</code> to indicate that
an item is to be repeated any number of times, i.e. zero or more
times, <code>+</code> to indicate that an item is to be repeated one or more
times, and <code>?</code>  to indicate that an item is optional (i.e. repeated
zero times or once).  For example:</p>

<pre>string = '"' ('"'! visible)* '"'
number = digit+
call = function "(" arguments? ")"
</pre>

<p>The <code>string</code> rule specifies that a string consists of a double
quote followed by any number of visible characters followed by a closing double
quote.  The <code>number</code> rule specifies that a number consists of one or
more digits.  The <code>call</code> rule specifies that a call consists of a
function name followed by brackets, and the brackets may optionally contain
arguments.</p>

<p>The three expressions <code>x*</code>, <code>x+</code>, <code>x?</code> act
exactly as if defined by:</p>

<pre>xs = x xs / ""
xp = x xs
xq = x / ""
</pre>

<p>This means that <code>x*</code> or <code>x+</code> will accept as
many <code>x</code>'s as are present in the input.  In all three cases, in line
with the fact that the choice operator does no backtracking by default, if
progress is made on a final <code>x</code> without success, the whole
expression fails.  Postfix operators bind tighter than sequencing.</p>

<h3 id="lookahead">Lookahead</h3>

<p>There are three lookahead operators, the <dfn>try</dfn> operator
<code>[...]</code>, the <dfn>has</dfn> operator <code>&amp;</code> and the
<dfn>not</dfn> operator <code>!</code>.</p>

<p>The try operator is specified using square brackets.  In an expression
<code>[x]</code>, the subexpression <code>x</code> is parsed speculatively.  If
it succeeds, parsing continues as normal.  If it fails, the parser backtracks to
the point just before <code>x</code>.  The try operator is usually used to
indicate the point at which the parser should commit to an alternative.  For
example:</p>

<pre>statement = [identifier "="] expression / ...
</pre>

<p>This specifies that the parser should commit to the first alternative only
after matching the equal sign.  If an error occurs before that, e.g. an
identifier has been matched but there is no equal sign, backtracking is done by
resetting the parser to the point before the identifier, and the next
alternative is tried.</p>

<p>The has <code>&amp;</code> and not <code>!</code> operators represent
positive and negative lookahead. In the PEG notation, <code>&amp;</code> and
<code>!</code> are prefix operators, but in Pecan, they are postfix, to make the
syntax of rules simpler and more uniform and, in particular, to avoid precedence
issues which would arise if there were both prefix and postfix operators.</p>

<p>With <code>x&amp;</code> or <code>x!</code>, the expression <code>x</code> is
parsed speculatively to check whether it appears next in the input or not.
Whether the expression <code>x</code> succeeds or fails, the parser backtracks
to the beginning. Then <code>x&amp;</code> succeeds if the parsing of
<code>x</code> succeeded, whereas <code>x!</code> succeeds if the parsing of
<code>x</code> failed.  For example:</p>

<pre>statement = (type identifier)&amp; declaration / assignment
</pre>

<p>The parser looks ahead to see if there is a type and identifier next in the
input.  If there is, the parser backtracks to the point before the type, but
continues with the same alternative and parses a declaration.  If the lookahead
fails, an assignment is parsed.  A negative example is:</p>

<pre>string = '"' ('"'! visible)* '"'
</pre>

<p>This says that a string contains any visible character other than a double
quote.  The bracketed expression only tries to match a visible character if a
double quote does not appear next in the input.</p>

<p>The postfix <code>x&amp;</code> and <code>x!</code> operators bind more
tightly than sequencing.</p>

<p>Within has and not operations <code>x&amp;</code> or <code>x!</code>, actions
are switched off during the processing of <code>x</code>.  Within a try
operation <code>[x]</code>, actions are delayed by storing them symbolically. If
<code>x</code> succeeds, the delayed actions are performed. If <code>x</code>
fails, the delayed actions are discarded.</p>

<p>The fact that actions are not performed during speculative parsing restricts
the possible context sensitive aspects of a language, which can only depend on
actions outside of lookahead constructs.</p>

<h3 id="characters">Characters</h3>

<p>Character notations are used to match input text. Double quotes indicate a
string of characters, which are matched in sequence. For example:</p>

<pre>keyword = "int" / "if" / "while" / ...
</pre>

<p>The string <code>"int"</code> only matches if all three characters appear in
the input in sequence, i.e. it is equivalent to <code>["i" "n" "t"]</code>. The
square brackets indicate that if a string matches only partially, the input
position returns to the beginning of the string, and other alternatives can be
tried.</p>

<p>This implicit lookahead is included in the string notation partly because it
tends to improve error messages, and partly to avoid any ambiguity over the
number of characters in a string, as explained in the <a
href="http://utf8everywhere.org/">UTF-8 everywhere manifesto</a>. Any Unicode
characters can be used in a string, since grammars are assumed to use UTF-8:</p>

<pre>pi = "&#960;"
</pre>

<p>Single quotes indicate a set of alternative characters, any one of which can
be matched.  For example:</p>

<pre>op = '+-*/'
number = '0123456789'+
</pre>

<p>The expression <code>'+-*/'</code> matches any one of the four arithmetic
operator characters, and is equivalent to <code>'+' / '-' / '*' / '/'</code>.
The expression <code>'0123456789'</code> matches any digit.</p>

<p>Although the single quote set notation may contain Unicode characters, each
code point is treated as a separate character. In any situation where graphemes
(multiple code points combined to produce a single visible character) might
cause ambiguity, the string notation should be used, e.g.</p>

<pre>x = "é" / "è"
</pre>

<p>Single characters can be represented using either single or double
quotes. For example:</p>

<pre>assign = id '=' exp
block = "{" statements "}"
double_quote = '"'
single_quote = "'"
</pre>

<p>There is no escape convention within single or double quotes.  To represent
control characters, or to represent Unicode characters using plain text,
integers are used.  An integer on its own represents a single character, using
its decimal character code.  For example:</p>

<pre>pi = 960
newline = 13? 10
</pre>

<p>If an integer is used which starts with a zero digit, then the character
code is in hexadecimal, using <code>a</code> to <code>f</code> or
<code>A</code> to <code>F</code>, e.g.</p>

<pre>pi = 03C0
newline = 0d? 0a
</pre>

<p>A set of characters can be specified using a Unicode range.  For
example:</p>

<pre>digit = '0'..'9'
letter = 'a'..'z' / 'A'..'Z'
visible = ' '..'~'
ascii = 0..127
</pre>

<p>Each argument to the range operator <code>..</code> must be a single
character, represented using single quotes or double quotes or an integer.</p>

<p>When representing individual characters using numbers or ranges,
<dfn>character</dfn> means Unicode code point.  No account is taken of whether
the code point is a valid character, or how characters are combined in grapheme
clusters. Any processing such as normalization must be handled by actions.</p>

<p>Common sets of characters can be specified using Unicode general categories.
The names <code>Uc, Cc, Cf, Cn, Co, Cs, Ll, Lm, Lo, Lt, Lu, Mc, Me, Mn, Nd, Nl,
No, Pc, Pd, Pe, Pf, Pi, Po, Ps, Sc, Sk, Sm, So, Zl, Zp, Zs</code> are provided.
The name <code>Uc</code> represents all Unicode characters
(<code>0..1114111</code>), and the others are the standard two-letter
abbreviations for the Unicode general categories which partition all the code
points.  For example:</p>

<pre>letter = Lu / Ll / Lt / Lm / Lo
digit = Nd
connector = '_'! Pc
visible = (Cc/Cn/Co/Cs/Zl/Zp)! Uc
</pre>

<p>The Unicode categories starting with <code>L</code> are letters.  The letter
rule allows any kind of letters: upper case, lower case, title case, modifier
and other. Categories starting with <code>N</code> are number characters, of
which <code>Nd</code> is the set of decimal digits.  The connector rule uses the
<code>Pc</code> connector punctuation category, but excludes the ASCII
underscore. The rule for <code>visible</code> excludes character code points
which are unassigned, private, surrogate, controls, or line or paragraph
separators.</p>

<p>The empty string <code>""</code> always succeeds, and the empty character
set <code>''</code> always fails because it has no matchable characters.  They
can occasionally be useful:</p>

<pre>succeed = ""
fail = ''
</pre>

<h3 id="tags">Tags</h3>

<p>Tags support token parsers, where there is a separate scanner. The input is
thought of as an array of tokens instead of an array of characters. Each tag
represents a specific kind of token.</p>

<p>A tag is represented as a symbol consisting of the <code>%</code> character
followed by a name.  For example, a token-based parser for a simple calculator
with no brackets might look like this:</p>

<pre>sum = term (%plus term @2add / %minus term @2subtract)* end
term = number (%times number @2multiply / %over number @2divide)*
number = %number @number
...
</pre>

<p>The parser matches tags in the same way as characters.  In this case, the
input is an array of tokens with tags <code>%number</code>, <code>%plus</code>,
<code>%minus</code>, <code>%times</code>, <code>%over</code>.</p>

<p>The tag <code>%</code> with no name is matched only at the end of the input.
This is equivalent to using <code>Uc!</code> in a scanner.</p>

<p>For greater readability, some tags can be represented as strings.
The simple calculator could be rewritten as:</p>

<pre>sum = term ("+" term @2add / "-" term @2subtract)* end
term = number ("*" number @2multiply / "/" number @2divide)*
number = %number @number
...
</pre>

<p>Any string used must be given a definitions which relates it to a named
tag:</p>

<pre>"+" = %plus
"-" = %minus
"*" = %times
"/" = %over
</pre>

<p>This allows string tags to be associated with an enumerated type in external
code when a parser is generated.</p>

<!--
Instead of the generated code being given an array of tokens, calls are made to
an external function which provides the next tag. This can be used to cope with
non-context-free features of languages. For example, take the infamous situation
with the C language where parsing depends on recognizing identifiers which have
been defined as type names using <code>typedef</code>.  A C scanner which is
independent of parsing may emit tokens with an identifier tag, which may or may
not turn out later to be type names.  The external functions for a C parser
would include symbol table construction, and could change the tags of tokens
given to the parser, according to those symbol tables.</p>
-->

<h3 id="actions">Actions</h3>

<p>Actions allow a parser to operate on values or data structures, using a
stack, to produce an output.  An action is a symbol which consists of
the <code>@</code> character followed by a number followed by a name.  If there
is no number, it is assumed to be zero.  For example:</p>

<pre>sum = term (plus term @2add)*
</pre>

<p>Suppose that the <code>term</code> rule pushes a single item onto the stack,
and the <code>plus</code> rule doesn't affect the stack. The <code>@2add</code>
symbol in the <code>sum</code> rule indicates that two items should be popped
off the stack, the <code>add</code> action should be performed, which creates a
new item, and the new item should be pushed onto the stack.  A parser as a whole
ends with a single item on the stack, which is the output from the parsing
process.</p>

<p>The stack is provided and manipulated by external code when a full parser is
generated. An action symbol such as <code>@2add</code> is translated into the
execution of a fragment of external code.  The parser could be part of a
calculator, in which case the <code>add</code> action might add two numbers.
Alternatively, the parser might become part of a compiler, in which case the
<code>add</code> action might combine two expression trees into a larger tree.
When executing a grammar symbolically for testing, an action simply causes the
text of the action to be printed.</p>

<p>An action has access to the characters or tokens from the input which have
been matched since the previous action.  For example:</p>

<pre>number = digit+ @number
</pre>

<p>The action <code>@number</code> creates a new item from the digits which
have just been matched, and pushes it onto the stack.  There is a further
convention which allows matched characters to be discarded.  For example:</p>

<pre>spaces = ' '+ @
</pre>

<p>An <code>@</code> sign on its own causes any recently matched characters to
be discarded by the parser, without any external code being executed.</p>

<p>A left hand alternative can begin with an action, for example:</p>

<pre>@a x / y
</pre>

<p>The action <code>@a</code> is only performed if <code>x</code> succeeds, or
if it progresses before failing. If <code>x</code> fails without progressing in
the input, the action <code>@a</code> is not performed, and the next choice
<code>y</code> is tried. To avoid the need to undo actions, the action
<code>@a</code> is delayed. It is performed when <code>x</code> progresses, or
discarded if <code>x</code> fails without progressing.</p>

<p>There are severe consistency restrictions on actions.  For example, each
alternative in a choice must create or consume the same number of output items.
Within a repetition, i.e. <code>x*</code> or <code>x+</code> or <code>x?</code>,
the inner expression <code>x</code> must have no net effect on the number of
output items.  The top level rule must yield a single output item.</p>

<p>The compensation for these severe restrictions is that Pecan can carry out
strong consistency checks on grammars, ensuring that every expression has the
right effect on the output stack, that the stack never underflows, and that a
single output item is produced overall. There is also the advantage that actions
are an integral part of the grammar, and are preserved during
transformations.</p>

<p>Scanners are normally thought of as producing a sequence of tokens.  But in
Pecan, even a scanner must produce a single output item.  A scanner can be
defined like this:</p>

<pre>tokens = @tokens token+ end
token = id @1id / number @1number / ...
</pre>

<p>The <code>@tokens</code> action creates an output item representing the list
or array of tokens to be generated, initially empty.  Each action such as
<code>@1id</code> takes the recently matched characters, creates a token from
them, and adds it to the list.</p>

<!--
<p>To help with generating a parser, an action name can contain arguments as
well as a function name, separated by dots.  For example, <code>@1id</code>
generates a call of the form <code>id(...)</code>, whereas
<code>@1token.ID</code> creates a call <code>token(ID,...)</code> allowing a
more generic function to be used.  An action <code>@5node2.5.1.3</code>
generates a call <code>node2(5,1,3,...)</code> where <code>node2</code> might be
a function which creates a node with two children, given the number of previous
nodes to pop off the output stack, and the indexes of the ones to be treated as
children.  This provides a fairly generic way to deal with discarding of output
nodes, and also allows all the relevant nodes to be available when the new node
is created, e.g. to work out the range of source text covered.</p>

<p>Finally, an argument may be a backquote tag such as <code>"+"</code> which
will automatically be converted into a suitable tag name.  For example,
<code>@1token."+"</code> is equivalent to <code>@1token.PL</code>. This is
useful in a scanner, where the corresponding parser uses backquote tags for
maximum readability.</p>
-->
<h3 id="errors">Errors</h3>

<p>By default, a parser produces an error message which points to the furthest
position reached in the input text, other than in lookaheads, but which gives no
details.  For example, suppose this rule is being parsed:</p>

<pre>sum = number ("+" number / "-" number)
</pre>

<p>Then a message like this might be produced for an incorrect operator:</p>

<pre>Error on line 1:
40~2
  ^
</pre>

<p>Markers can be added to parser rules, to describe the items which would have
allowed parsing to continue.  A marker is a symbol consisting of
the <code>#</code> character followed by a name. For
example, suppose the rule above is changed to:</p>

<pre>sum = number (#plus "+" number / #minus "-" number)
</pre>

<p>Then, for an incorrect operator, the expressions <code>"+"</code>
and <code>"-"</code> both fail, so the error message produced becomes:</p>

<pre>Error on line 1: expecting minus, plus
40~2
  ^
</pre>

<p>When a marker is encountered, it is associated with the current position in
the input. It records something that the parser is expecting at that point. If
progress is made past that input position, the markers are cleared. When the
parser encounters an error, the set of things that the parser was expecting at
that point can be reported. Duplicates are removed, for example suppose the rule
is changed to:</p>

<pre>sum = number ("+" #operator number / "-" #operator number)
</pre>

<p>Then the error message becomes:</p>

<pre>Error on line 1: expecting operator
40~2
  ^
</pre>

<p>One way to ensure that all the items which could possibly allow parsing to
continue are reported is to write separate rules to describe low level features
of a grammar involving primitive character matchers, i.e. strings, sets,
character codes, or Unicode identifiers, and begin each with a marker.  For
example, a parser for a programming language might contain rules such as:</p>

<pre>plus = #operator "+" " "* @
letter = #letter (Lu / Ll / Lt / Lm / Lo)
number = #number ('0'..'9')+ @number " "* @
newline = #newline 13? 10 @
end = #end Uc!
</pre>

<p>The <code>plus</code> rule specifies that the plus sign is to be described as
an operator in error messages, that it may be followed by optional spaces, and
that the plus sign and spaces are discarded.  The <code>letter</code> rule
applies an error marker to a choice of Unicode categories, to avoid having to
attach a marker to each one individually.</p>

<p>With these rules, an error message like the one above might be generated,
saying that an operator is expected. The fact that an extra digit on the
preceding number, or a space, could also have allowed parsing to continue is not
reported. That is because individual digits and spaces are not marked in the
rules.</p>

<h2 id="testing">Testing</h2>

<p>During developing of a grammar, it can be tested by symbolic execution
without having to generate a parser. Testing is carried out by typing a command
of the form:</p>

<pre>pecan [-trace] [grammarfile] testfile [line]
</pre>

<p>The test file contains any number of tests. It may contain the grammar, or
fragments of the grammar, or variations on the grammar, making it
self-contained. Alternatively, the grammar can be given in its own file, to form
a basis for the tests in the test file. The <code>-trace</code> option switches
on tracing, so that the individual steps taken are displayed.


 When a test fails, its line number is
reported in the error message. That line number can then be used to re-run
testing, but with only that

    file contains the grammar and any number of tests.  Each test is
preceded by a line containing just equal signs, and consists of input text,
then a line containing just minus signs, then the expected output.  The
expected output has one line for each external action the generated code
would make, e.g.:</p>

<pre>sum = number / number "+" number @2add
number = ("0".."9")+ @number
==========
2
----------
number "2"
==========
42
----------
number "42"
==========
2+40
----------
number "2"
number "40"
add
</pre>

<p>For each action, its name is given, followed by the matched but unused input
characters when the action is performed, if any.</p>

<p>There is an escape convention which can be used in both the input and
expected output portions of tests.  This allows the input or output to contain
control characters or unicode characters as plain text.  A backslash followed
by digits represents a character by its decimal code, or by its hex code if the
code starts with zero.  Two backslashes are used to represent a single
backslash, and a backslash followed by any other character removes that
character.  In particular, a backslash followed by a space can be used as a
separator, and a backslash followed by a newline can be used to cancel the
newline.  For example, given that <code>960</code> is the decimal code for the
character &#960;, then:</p>

<ul style="list-style-type:none;">
<li><code style="display:inline-block;width:5em;">\960x</code>
is &#960; followed by x</li>
<li><code style="display:inline-block;width:5em;">\960\ 5</code>
is &#960; followed by the digit <code>5</code></li>
<li><code style="display:inline-block;width:5em;">\\960x</code>
is the five characters <code>\960x</code></li>
<li><code style="display:inline-block;width:5em;">...\13\</code>
is a line ending in CR instead of LF</li>
</ul>

<h2 id="checks">Consistency Checks</h2>

<p>A number of checks are performed on a grammar which help to ensure
correctness, or at least consistency. As well as obvious checks, such as syntax
checking of the grammar, and checking that each rule name is defined exactly
once, the following further checks are done:</p>

<h3 id="type">Type Checking</h3>

<p>A grammar is checked to see if it contains character notations, or tags, but
not both.  If it contains character notations (<code>"..."</code> or
<code>'...'</code> or numbers or ranges) then the input to be parsed by the
grammar is text. If the grammar contains tags, then the input to be parsed
consists of tokens.</p>

<p>If the range operator is used, there is a check that each operand is a
character, i.e. a one-character string or one-character set, or a name which
refers to one of those:</p>

<pre>digit = "0" .. "9"
letter = 'a' .. 'z' / 'A' .. 'Z'
control = NUL .. US
NUL = 0
US = 31
</pre>

<p>These checks are implemented by classifying every expression in the grammar
according to whether or not it refers to a single character.</p>

<h3 id="loop">Loop Checking</h3>

<p>Pecan checks that the grammar contains no obvious infinite loops.  More
precisely, Pecan checks for left recursion.  The simplest example is where a
rule mentions its own name at the start of its right hand side, or at the start
of one of its alternatives:</p>

<pre>sum = sum "+" term / term
</pre>

<p>In a Pecan grammar, a rule of this form leads to an immediate infinite loop,
and so is reported as an error.</p>

<p>Indirect left recursion is also detected.  That is where two or more rules
mention each other at the beginning:</p>

<pre>expression1 = expression2 ...
expression2 = expression1 ...
</pre>

<p>Some less obvious cases of left recursion are also detected, e.g.</p>

<pre>statement = label* statement
</pre>

<p>Although <code>statement</code> does not mention itself right at the
beginning, the expression <code>label*</code> may succeed without any input
being matched, and therefore an infinite loop ensues.</p>

<p>This check is implemented by finding out for each expression in the grammar
whether or not it is optional, i.e. whether it can succeed without making any
progress by matching some input.  Then, for each rule, the rule names which it
starts with are recorded, taking account of initial optional expressions.
Finally, any loops within these recorded names are detected.</p>

<!--
<h3 id="token">Token Checking</h3>

<p>For grammars which have text as input, a check is made that all tokens
produced are non-empty, so that continual progress is made through the
characters in the input.  For example:</p>

<pre>id = letter+ @identifier
letter = 'a' .. 'z'
</pre>

<p>Here, it is clear that by the time the accept action
<code>@identifier</code> is reached, at least one letter has been matched from
the input.  On the other hand, suppose the <code>id</code> rule was:</p>

<pre>id = letter* @identifier
</pre>

<p>This causes an error message, because <code>@identifer</code> can be reached
without matching any input characters.  However, if this <code>id</code> rule
is always used in a context where at least one character has already been
matched, i.e. the <code>id</code> rule refers to the remainder of a token
rather than a whole token, then there is no error.</p>

<p>This check is implemented by making as many deductions as possible about
positions in the grammar at which input characters have definitely been matched
since the previous token.  This is a conservative check - it is possible for a
pathological grammar to produce an error message even though no empty token
would ever be created in practice.</p>

<p>There is a further check that a scanner makes no attempt to read past the
end of the input.  For example, suppose there is a scanner rule like
this:</p>

<pre>tokens = token+ '' @end
token =  space / identifier / keyword / operator / punctuation
</pre>

<p>This is fine, because all that follows the end of text <code>''</code> is an
action, not any attempt to match any characters.  However, suppose the
rule is:</p>

<pre>tokens = token+
token = space / identifier / keyword / operator / punctuation / end
end = '' @end
</pre>

<p>This does cause an error, because after recognising the end of text, the
scanner could continue to look for more tokens.</p>
-->
<h3 id="output">Output Checking</h3>

<p>Actions are assumed to treat output items in a stack-like manner.  Checks are
made to guarantee consistent handling of output items.  One check is that, at
the end of processing, there is exactly one output item.  For example:</p>

<pre>example = "1" @n "2" @n "3" @n @2add
</pre>

<p>In this case, three items are pushed onto the stack, and two of them are
combined into one by the <code>add</code> action, but that leaves two items on
the stack at the end of processing, so an error is reported.</p>

<p>A second check is that the stack never underflows.  For example:</p>

<pre>example = "1" @n @2add
</pre>

<p>Here, when the <code>add</code>action is reached, there is only one output
item on the stack, whereas two previous output items are supposed to be passed
to <code>add</code>, so an error is reported.</p>

<p>To implement these checks, Pecan calculates the overall number of items added
to the stack, possibly negative, for each expression in the grammar.  It also
calculates a low water mark value for every expression in the grammar,
representing the number of items that are needed on the stack during processing
of the expression.  It then checks that for the first rule in the grammar, which
represents the final result, one item is added, and the low water mark isn't
negative.</p>

<p>In order to carry out these checks, a very uniform approach to actions has
to be taken.  For example, Pecan checks that each alternative in a choice adds
the same number of items to the stack, and that where there is a repetition
operator, e.g. <code>x?</code> or <code>x*</code> or <code>x+</code>, the
subexpression <code>x</code> has no overall effect on the number of items on
the stack, so that the number of times <code>x</code> is repeated doesn't
have any overall effect on the stack size.</p>

<p>These checks are conservative, i.e. there could be pathological grammars
which always correctly produce one output item in practice, but which don't
pass the checks.</p>

<h2 id="java">Generating Code</h2>

<p>Code is generated as a bytecode with an interpreter.  This is somewhat
similar to the table-driven techniques used in some other parser
generators.  Advantages of using the bytecode approach are:</p>

<ul>

<li>it allows the same implementation approach to be used in different
target languages, regardless of their features</li>

<li>it allows mismatches to be treated using a lightweight version of exception
handling</li>

<li>it allows calls, which are very frequent, to be implemented in a simple
and efficient way using a custom stack</li>

<li>it makes various optimisations such as inlining and tail calls easier to
implement</li>

</ul>

<p>In a straighforward bytecode implementation, with no optimization, each
expression is compiled into a bytecode function which succeeds or fails.  A
choice <code>x / y</code> might be encoded as:</p>

<pre>EITHER &amp;X OR &amp;Y RETURN
</pre>

<p>Here <code>X</code> stands for the code generated from subexpression
<code>x</code>, and <code>&amp;X</code> stands for the address of
<code>X</code>, stored in two bytes.  The <code>EITHER</code> opcode remembers
the input position and calls <code>X</code>.  The call returns to the
<code>OR</code> opcode, which checks whether any progress was made in the input.
If not, it calls <code>Y</code>, which returns to the <code>RETURN</code>
bytecode, which returns from the execution of <code>x / y</code>.</p>

<p>Immediately, a tail-call optimization can be added.  Instead of calling
<code>Y</code> and then returning, the <code>OR</code> opcode can jump to
<code>Y</code>, so the final <code>RETURN</code> opcode is not needed. Also, it
is reasonable to assume that the code for the two subexpressions <code>x</code>
and <code>y</code> immediately follows the code for <code>x / y</code>. The
sequence becomes:</p>

<pre>EITHER OR &amp;Y
</pre>

<p>Now, the <code>EITHER</code> opcode assumes that <code>X</code> immediately
follows <code>&amp;Y</code>.  The full set of translations at this point,
without further optimisation, is:</p>

<pre>r = x    RULE frame         frame = amount of call stack needed
r        GO &amp;R              identifier: jump to its rule
x / y    EITHER OR &amp;Y       call Y if X fails without progress
x y      BOTH AND &amp;Y        call Y if X succeeds
x?       REPEAT ONCE        call X: fail without progress becomes success
x*       REPEAT MANY
x+       DO THEN REPEAT MANY
[x]      LOOK TRY
x&amp;       LOOK HAS
x!       LOOK NOT
#m       MARK M
%t       TAG T
'abc'    SET 3 a b c
"abc"    STRING 3 a b c
10       STRING 1 10
a..c     RANGE a c
Lu       CAT Lu
@        DROP
@f       ACT f
</pre>

<p>An important optimisation when generating code is to treat a choice between
alternatives as a switch, where possible.  For example, suppose a grammar
contains a rule:</p>

<pre>token = id / number / string / ...
</pre>

<p>The alternative to be chosen depends on the next character in the input.  An
alternative such as <code>number</code> starts with a character from a set such
as <code>'0123456789'</code>.  Sometimes, the set may contain a single
character, e.g. the alternative <code>string</code> may start with
<code>'"'</code>.  If these sets are disjoint, or can be made disjoint, then
a switch can be used to implement the choice.</p>

<p>Pecan takes a very uniform approach to this situation.  First, the entire
grammar is analysed to find suitable disjoint sets of characters.  Some of the
sets may contain just one character. For example, set <code>0</code> might
represent the upper case letters, set <code>1</code> the lower case letters,
set <code>2</code> might represent <code>'0123456789'</code>,
set <code>3</code> the character <code>'"'</code>, and so on.  Each character
is classified into one of these sets.  This is done on the whole input as a
separate pass before parsing begins, to avoid repetition as a result of
backtracking.  The <code>token</code> rule can then be represented as a
switch:</p>

<pre>SWITCH &amp;id &amp;id &amp;number &amp;string ...
</pre>

<p>Both upper and lower case letters will trigger a jump to the <code>id</code>
rule, a digit will cause a jump to the <code>number</code> rule,
the <code>'"'</code> character causes a jump to the <code>string</code> rule,
and so on.  This technique covers non-ascii characters and Unicode general
categories as well as simple ascii characters.</p>

<h2 id="side">Side Effects</h2>

<p>A grammar is linked with external code to make a practical parser.  The
actions in the grammar become calls to this external code.  As well as
manipulating the stack of output items, the external code may very well maintain
a state of some kind, and the actions may have side effects on that state.</p>

<p>Such side effects are awkward, because they interact badly with lookahead. On
the face of it, some system of undoing side effects would have to implemented,
so that backtracking could be guaranteed to have the right effect.  As well as
returning to a previous position in the input, backtracking would need to undo
any effects caused by actions. In Pecan, at present, this isn't done. Instead,
actions are switched off during lookahead. It is assumed that lookahead is only
used to ask simple syntactic questions, in order to resolve ambiguities.</p>

<p>On the other hand, if side effects were not allowed to have any effect on
parsing, some features of some languages couldn't be handled.  The most infamous
example is the C and C++ languages, where type names introduced using
<code>typedef</code> need to be categorized properly to parse programs, and yet
they can only be recognised after parsing their definitions, by maintaining a
symbol table during parsing.  This is handled in Pecan by allowing side effects
to change the types of tokens which haven't yet been read.</p>

<h2 id="transforms">Transforms</h2>

<p>The Pecan grammar language is intended to be sufficiently simple and precise
that an equational theory of transforms can be developed.  This theory could be
used to develop and justify automatic optimisations, or to build an assistant
which would suggest and check user-driven transforms while developing grammars.
The theory might include equations such as:</p>

<pre>(x y) z == x (y z)
(x / y) / z == x / (y / z)
x+ == x x*
x* = (x+)?
[[x] y] z == [x y] z
[x y] z / [x u] v == [x] ([y] z / [u] v)
(x / y) z = x z / y z
x y / x z == x (y / z)  <em>under suitable conditions</em>
x / y == y / x          <em>under suitable conditions</em>
</pre>

<p>This theory has yet to be established, but it is envisaged that it would
include the output building and error handling aspects of grammars.</p>

<h2 id="pecan">The Pecan Grammar</h2>

<p>Here is a Pecan parser for the Pecan language itself:</p>

<pre>pecan = skip rules Uc!
rules = rule (rules @2add)?
rule = id equals expression newline skip @2rule

expression = term (slash expression @2or)?
term = factor (term @2and)? / marker term @2handle
factor = look atom @2look / not atom @2not / atom repetition?
repetition = opt @2opt / any @2any / some @2some
atom = id / action / tag / range / back / bracket

id = letter alpha* @id gap
action = '@' (digit* letter alpha* @act / @drop) gap
tag = "%" letter alpha* @ask gap
marker = "#" letter alpha* @mark gap
range = text (".." skip @ text @2range)?
text = number / string / set
back = sb rule se @3back
bracket = rb rule re @3bracket

number = (("1".."9") digit* / "0" hex*) @number gap
string = '"' ('"'! visible)* '"' @string gap
set = "'" ("'"! visible)* "'" @set gap

equals = "=" infix
slash = "/" infix
look = "&amp;" prefix
not = "!" prefix
rb = '(' prefix
sb = '[' prefix
opt = "?" postfix
any = "*" postfix
some = "+" postfix
re = ')' postfix
sb = ']' postfix

infix = skip @
prefix = @token skip @
postfix = @token gap @
skip = (space / comment / newline)*
gap = space* comment? continuation @
continuation = [nl skip &amp;'=/)]']?
newline = (10 / 13 10?) @
comment = ["//"] visible* &amp;newline
visible = (Cc/Cn/Co/Cs/Zl/Zp)! Uc
alpha = letter / digit
letter = Lu / Ll / Lt / Lm / Lo
digit = Nd
hex = digit / 'ABCDEFabcdef'
</pre>
<!-- Write a Haskell version of Pecan, then prove it. -->

<!--
<h2>Notes</h2>

<p>Error sets are reported in a standard order (TreeSet) so that alternatives
can be swapped by the optimiser if they start differently, e.g. if there are
alternatives x/y/z and y starts with a keyword whereas x and z start with an
id, then the optimiser might transform to y/(x/z) to allow a case switch on the
outer choice, while the alternatives of the inner choice are kept in order.</p>

<h2>Implementation</h2>

<p>Generate direct code: problems are (b) incremental or
stream (c) no tail-call optn.  Possible optimisations are: (a) inlining
especially for short non-recursive defs but also of initial calls to avoid
call-downs, and final calls to avoid tail-calls (although it isn't clear there
are many true tail-calls).  (b) Case switching for alternatives. (c) @ID@
becomes SKIP.  For case-switching in a parser, there can just be a case per
type (requires us to go back to constants!)  For case switching in a scan*,
there is a case per character, or a case per kind if compactness is needed,
where a kind is a minimal set.</p>

<p>Full table-driven approach: problems are (a) not much opportunity for
compiler optn (b) case switch per 'line of code' (c) not easy to
understand or debug</p>

<p>Want compromise.  A case per rule doesn't work, because in</p>

<pre>case P:
  ok = CALL(Q);
  if (ok) ok = CALL(R);
</pre>

<p>the simulated call to Q, using our own stack, needs a return address between
the two statements, so the case labels are not enough.</p>

<p>A table where each position in the code represents an expression can
work:</p>

<pre>P:  AND P2 P3
P2: CALL Q
P3: CALL(R);
</pre>

<p>A problem is (again) what is the return address to put on the local stack
while calling Q? (It needs to be between the halves of the AND.)</p>

<p>Maybe a more-or-less direct translation of the direct code could work:</p>

<pre>P:  CALL Q   (represents ok = q() )
    IF P2         (represents if (ok) ok = r() )
    CALL(R)
P2: END           (represents if (!ok) out = out3; )
</pre>

<p>Case switch optimisation: Find all the starter sets for the alternatives,
and check overlap.  If there are sufficient <i>distinct</i> sets, try for a
case switch, treating an error handler as "everything else".  Sort, combining
alternatives with overlapping sets, making sure swapping is valid, and leaving
an error handler last.  Aside from the error-handler, work out which new
alternative is best put last to act as default, to save cases.  If all the
cases bar one lead to a small enough number of cases, use a character or
token-type switch.  Otherwise, use a lookup to categorise the next character
and switch on sets.</p>

<h1>Old Pecan Notes</h1>

<p>Soft failure means input position doesn't change (though it may have been
reset by backtracking).  Because of rule "commit only if input consumed", if
soft failure, output may have to be reset.  In particular, apart from backtrack
or lookahead features, in a sequence, if it fails on second or subsequent item,
but without eating any input, then output position needs to be reset.</p>

<p>The <code>failWith</code> method usually returns the list of symbols which
have been tried but have failed, so that a standard message such as "expecting
name, number or string" can be generated.  However, if the failure is due to an
error token in the input, that is taken to be a scanner error, and is reported
in preference to the expected symbol.  If an error symbol is expected, then
that is reported in preference to any previous expected symbols, even ones
further on in the input.</p>

<h2>Scanner Implementation</h2>

<p>The set expressions are analysed.  The smallest intersections are computed
and these form the base sets.  Each set, possibly excluding any remaining
single characters, is expressed as a union of the base classes.  A table is
computed where each character is looked up to find its base class.  Preferably,
there are up to 8 or 16 or 32 or 64 bases, and the result can be a one-bit
number.</p>

<h2>Incremental Scanning</h2>

<p>Here are some restrictions so that a scanner can be used for incremental
scanning, as in a syntax highlighting editor.</p>

<ul>

<li>Each token is a non-empty exact substring of the source text, with a
classifying tag. The tokens exactly divide up the text, each starting where the
previous one ends.  White space tokens can be discarded at some higher
level.</li>

<li>The scanner never fails.  Error tokens are produced instead, distinguished
by tag.  Scanning continues as normal afterwards.  This allows syntax
highlighting of incorrect programs, and also allows scanning errors to be
reported during parsing so that, to the user, scanning and parsing appear to be
a single pass.</li>

</ul>

<p>With any rescanning mechanism, the amount of lookahead must be known:</p>

<ul>

<li>The amount of lookahead at the end of a token must be known.  The lookahead
may be globally limited (e.g. always at most one character) or limited per
token type (two characters for a string literal).  Otherwise, the scanner must
keep track of a "high water mark", i.e. the furthest point in the text that was
read from the start of scanning up to producing the current token, and use that
to form a global lookahead limit, or encode the lookahead in the token type.
Perhaps a max lookahead can be declared, and if the actual lookahead ever
exceeds that, an error or change of strategy is indicated.  (The default can be
simply to track the max lookahead, and an option can switch on a lookahead per
token.)</li>

</ul>

<p>Re-scanning after a change of text consists of finding the rightmost
unaffected token, i.e. the end of the token plus the amount of lookahead does
not reach the text which has changed.  Re-scanning continues until the text
resynchronises, i.e. a new token 'exactly matches' an old one.</p>

<p>Simple scanners can get away with a small fixed number of states.  For
example, JSP mixes HTML with Java, or splitting multi-line (but not nested)
comments into one-line tokens to speed up incremental highlighting.  For that,
you need:</p>

<ul>

<li>The scanner can be in one of a small number of 'memory free' states.  Given
the current state and position in the source text, the next token can be found
purely by working forwards in the text.  In other words, there must be no
backtracking back past the start of a token.</li>

<li>Each tag (token type) completely defines an end state, which is the start
state for scanning the next token.  There is a fixed start state, and after
that the parser state is completely determined by the end state of the tag just
generated.  For example, if scanning JSP, there can be a Java state and an HTML
state.  The token tags distinguish between Java tokens, HTML tokens, and
transitional tokens in each direction.  This would include distinguishing
between Java whitespace or error tokens, and HTML whitespace or error
tokens.</li>

</ul>

<p>Having a fixed number of parsing states is no good for nesting situations.
Nested comments require a state which counts the depth of nesting.  Multiple
types of nested brackets require a state which keeps track of the whole
sequence of open brackets.</p>

<p>For interactive applications such as syntax highlighting, it is recommended
that very large tokens be avoided.  For example, multi-line comments can be
divided into one-line pieces (with tags "start of comment", "middle of
comment", "end of comment").  This needs an extra scanner state ("inside
multi-line comment").  If the comments are nested, it needs an infinite number
of states.  On the other hand, changing the start or end of a multiline comment
is likely to involve rescanning the whole token or the whole of the rest of the
source text anyway, and this is not necessarily slow, so maybe it is ok to
leave it.</p>

<p>An alternative is fully automated incremental scanning.  An interpretive
approach is used with a custom execution stack.  For each token, a position in
the execution stack is recorded.  Although an individual rule never goes below
its original starting point, and indeed always returns exactly to it,
nevertheless further processing after the rule may combine the stack and destroy
the information before the rule's starting point.  So, the stack needs to be
orgaised as a history stack.  When a rule returns, instead of popping an item
off the stack, a skip is pushed on the stack saying that part of the stack is
to be ignored.  See HistoryStack.java.  The state at the time a token is
created can be stored with it, instead of assuming the type can be used as a
state.</p>

<h2>Stream Scanning</h2>

<p>Stream scanning can be done using the re-scanning mechanism.  Scanning is
done a buffer of text at a time.  Each new buffer is regarded as a change to
the end of the text, adding some more.  For each buffer, all tokens where the
high water mark does not reach the end of the text can be emitted, before the
next buffer is processed.</p>

<h2>Incremental Scanning with Finite States</h2>

<p>It should be possible to check a normal grammer to see if it can be made
incremental with a fixed number of states rather than the more general history
stack mechanism.  Subexpressions of the grammar (rules) are classified as (a)
not producing any tokens or (b) producing one token at the end or (c) looping.
A looping rule cannot appear as a subrule of a larger expression, except in
alternatives of a defined rule.  A looping rule or alternative must consist of
a part which produces a token at the end, followed by the name of a defined
looping rule.  Each occurrence of a particular token type must be followed by
the same looping rule.  This should be enough to ensure that when a token is
produced, there is ony one tail-recursive return address on the execution
stack, and that is for a named rule which can be returned to later using just
the type of the previous token.</p>

<h2>Keywords</h2>

<p>Suppose a language has <code>int</code> as a keyword, but also allows
identifiers such as <code>intx</code> that begin with <code>int</code>.  Then
there appear to be two possibilities.  One is to use lookahead, the advantage
of which is that the rule for each keyword and the rule for identifiers can be
expressed separately:</p>

<pre>    token = int / ... / id
    int = "int" [non-letter-or-digit] ! #
    id = letter letter-or-digit* id#
</pre>

<p>Note, however, that the id rule must be the last alternative, to express the
fact that it excludes keywords, which does not fit in with the idea in PEG of
combining scanner and parser grammars, because then the id rule would have to
stand alone.  The other alternative, avoiding lookahead, is for the identifier
rule to be intertwined with the keyword rules, e.g.:</p>

<pre>    token = "int" ! key / ... / other_id
    key = letter-or-digit+ id# / #
</pre>

<p>Either way, it would be helpful to rely on a couple of simple optimisations.
One is common prefix elimination, e.g.</p>

<pre>    x y / x z    == &gt;    x (y / z)
</pre>

<p>This helps to make sure that alternatives begin with disjoint character
sets.  That allows a second optimisation, which is switching, e.g.:</p>

<pre>    'a' ... / 'b' ... / digit ... / ...
</pre>

<p>Instead of trying each alternative in turn, code is generated to (a) convert
each character into a small integer representing a disjoint character set, and
(b) do a switch on that integer to jump to the right alternative.</p>

<h2>Incremental Parsing</h2>

<p>To support this, a table-driven implementation is used, so that parsing can
be suspended, the current position in the execution stack can be changed, and
parsing can be resumed.  The execution stack is a history stack.</p>

<p>After parsing, each node carries with it the start and end positions in the
source text, a lookahead based on the high water mark (furthest position
forward in the source text on creation) and the parsing end-state
(i.e. position in the execution stack) at the moment of creation.</p>

<p>The input is an array of characters or tokens with a current marker in it.
The output is an array (stack) of actions with an end marker.  The actions
typically build nodes, but are not executed until parsing is complete, so that
backtracking can be done without side-effects.  When each output action is
pushed, the position in the execution stack is recorded with it.  After
parsing, the final execution stack is kept to support incremental
re-parsing.</p>

<p>To re-parse a region of source text which has changed, find the last output
action whose high water mark is before the changed text.  Take the next node,
and move to the parent until the node's range covers the changed text.  Use its
parsing start-state and start position to restart parsing.  Stop reparsing when
synchronisation occurs (same end position and parsing state (and hwm?) when an
output action is created).  The array of output actions consists of parts a, b
and c where b is the new part.  The nodes corresponding in part a can be kept
intact.  The actions in part b are used to create new nodes.  Where nodes in
part c had nodes in part b as subnodes, those subnodes need to be replaced
somehow.</p>

<p>One way is to scan part c (but this doesn't feel good because it is not
limited).  It is a rapid replay, simulating just the number of nodes beyond the
new ones, not their values.  However, it almost certainly means replaying to
the end of the file, past a lot of irrelevant stuff.</p>

<p>Another is keep track of the correspondence between old and new nodes by
replay both b and oldb (the old counterpart of section b) together.  Then
replace the old node in its parent by the new node.  From each node, you need
to be able to reach its parent, and its position within its parent.</p>

<p>To make this work, a lot of consistency checks are needed.  In particular,
if the execution stack ever goes below the point recorded in an output action,
this must be due to backtracking which will also remove the output action.</p>

<p>Each expression in the grammar forms a rule.  A rule may consume input, may
produce output, and may succeed or fail.  A failure is soft if no input has
been consumed, hard otherwise.  A backtracking operation (lookahead or failed
commit) must restore the input position, and output position, and execution
stack, to what it was.  A soft failure which consumes no input but produces
output must discard the output produced by restoring the output position to
what it was.  For example, suppose there is a sequence <code>x y</code> where
<code>x</code> is just an output action, and then <code>y</code> fails.  This
is a soft failure of <code>x y</code>, and any output produced by
<code>x</code> must be discarded.</p>

<h2>Interpretive Execution</h2>

<p>The purpose of custom interpretive execution is to have our own execution
stack, organised as a history stack which can be restarted at any point.  It
contains return addresses and saved local variables.  One choice for an
interpretive style of code seems to be this.</p>

<pre>SEQ x AND y AND z END

SEQ: push(out), call(p+1)   [[ = push(p+2), jump(*(p+1)) ]]
AND: if fail { out=pop(); return } else call(p+1)
call: if return address goes to END, TAILCALL

ALT: push(in), call(p+1)
OR: if succeed return; if (in > saveIn) return; call(p+1)
</pre>

<p>The AND instruction forms a return address for execution to return to after
executing the previous instruction.  The call() operation checks for the end of
an AND sequence, and does a tail call instead.  (An alternative would be to
have a different final version of AND, and/or to pre-process to change the last
ANDs to tail call versions.)</p>

<p>Alt: on success or hard failure of an alternative, return.  On soft failure,
try next.  Need to store 'in' to test for soft failure.  Don't need to store
'out' because each alternative which soft-fails restores 'out', as does the one
which hard-fails.</p>

<p>Seq: on failure, restore 'out' and return.  If all items succeed, return
success.  Need to store 'out' to restore it.  Restoring 'out' on hard failure
should be no problem, because parsing will crash or there is a surrounding
backtrack.</p>

<p>Try: save in, do x, if it fails restore in.  Don't need to restore out if x
restores out on failure.</p>

<p>Look: save in and out, do x, if it fails restore in.  If x succeeds, must
restore both in and out.</p>

<p>Opt: x? is essentially x / nothing.  If x succeeds, ok.  If x hard-fails,
ok.  If x soft-fails, report success (and out has been restored).  Must store
in to test for soft-fail.</p>

<p>Any: x* is x x* / nothing.  If x succeeds n times and then hard fails, ok.
If x succeeds n times and then soft fails, out is restored by the soft failure,
report success.  Need to store in to test for soft failure.</p>

<p>Some: x+ is x x*.  If x fails, ok.  If it succeeds, do as for Any.  Need to
store in.</p>

<p>Atom: If succeeds, moves in and adds SKIP to output.  Else soft-fails.
Cooperates in collecting expected bitset via global failPosition and
failTags.  Not needed for re-parsing?</p>

<p>Act: add given action to output.</p>

<p>IDEA: can we check for separability of the scanner from the parser?  Maybe
like this.  Take the whole grammar, and discard all the tree-like actions which
build nodes.  Radically simplify the resulting grammar down to its bare
essentials.  See if there is ever any backtracking past the creation of a token
which results in a different token or token type.</p>

<h2>Tutorial Topics</h2>

<p>Topics: grammar operators, left recursion, right recursion, iteration.
Stack-based actions.  Actions not done and undone if backtrack.  Actions can do
stuff or build trees.  Built-in scanning and drawbacks.  Separate scanner.  How
to join then up.  Incremental scanning.  Incremental parsing?</p>
-->

</body>
</html>
