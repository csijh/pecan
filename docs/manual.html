<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-gb" xml:lang="en-gb">
<head>
<title>Pecan Manual</title>
<style>pre { margin-left:40px; }</style>
</head>
<body>

<img src="pecan.png" width="300" height="65" />
<hr/>

<!--
TODO: generate

Have only one entry point. (Don't generate rules?)
Insist on all names distinct. (Or what? Prefix?)
Insist on first character or tag to make parser choose rule.
Option to add action to ACT opcode?
Generate: OP, ACT, add, MARK, id, TAG, id, CHAR, int


Have opcodes, actions, errors, tags, rules, bytes.
Want to connect parser to grammar via enumerations.
Template in any language contains e.g.
<opcodes %s=%d, >
In the printf-like string, %s is name, %d is number, other text is verbatim.
Verbatim text after last % is separator text, not printed after last entry.

TODO: interpreter

Rename the Interpreter class.
What about @i causing two switches?
OR have @0, @1, @2, ... opcodes.
How link up scanner enumeration of actions with parser enumeration of tags?
Can we specify external enumerations? (@a = 3  OR  %id = 2)
(Pecan chooses numbers for scanner actions, and for parser tags!)
(Same set, same spellings, sorted maybe? @ and % first # for end?)

-->

<h1>Pecan Reference Manual</h1>

<ul>
<li><a href="#intro">Introduction</a></li>
<li><a href="#install">Installation</a></li>
<li><a href="#notation">Notation</a>
<ul>
<li><a href="#rules">Rules</a></li>
<li><a href="#comments">Comments</a></li>
<li><a href="#parentheses">Parentheses</a></li>
<li><a href="#continuations">Continuations</a></li>
<li><a href="#choices">Choices</a></li>
<li><a href="#sequences">Sequences</a></li>
<li><a href="#repetition">Repetition</a></li>
<li><a href="#lookahead">Lookahead</a></li>
<li><a href="#text">Text</a></li>
<li><a href="#tokens">Tokens</a></li>
<li><a href="#actions">Actions</a></li>
<li><a href="#errors">Errors</a></li>
</ul>
</li>
<li><a href="#testing">Testing</a></li>
<li><a href="#checks">Consistency Checks</a>
<ul>
<li><a href="#type">Type Checking</a></li>
<li><a href="#loop">Loop Checking</a></li>
<li><a href="#token">Token Checking</a></li>
<li><a href="#output">Output Checking</a></li>
</ul>
<li><a href="#java">Generating Code</a>
<ul>
<li><a href="#code">Bytecode</a></li>
</ul>
</li>
<li><a href="#side">Side Effects</a></li>
</li>
<li><a href="#transforms">Transforms</a></li>
<li><a href="#pecan">The Pecan grammar</a></li>
</ul>

<h2 id="intro">Introduction</h2>

<p>Pecan is a tool for developing and checking grammars, and generating scanners
and parsers. It is aimed at parsers for programming languages, <a
href="https://en.wikipedia.org/wiki/Domain-specific_language">domain specific
languages</a>, and other unambiguous languages. It provides a precise grammar
notation, which is independent of implementation language, based on recursive
descent with lookahead. This manual describes Pecan version 1.0 which has these
features:</p>

<ul>

<li>a grammar is an executable prototype parser</li>

<li>output actions and error markers are included</li>

<li>transformations preserve actions and errors</li>

<li>many consistency checks are applied</li>

<li>there is explicit support for development and testing</li>

<li>scanner and parser generation is via bytecode</li>

<li>an interpreter for the bytecode can be written in any language</li>

</ul>

<p>The previous version of Pecan was 0.4. Changes are:</p>

<ul>
<li>actions are now allowed at the start of a left hand choice</li>
<li>actions are delayed in case they need to be discarded</li>
<li>error markers are no longer postfix</li>
<li>the range notation now supports strings</li>
<li>string tags use double quotes instead of backquotes</li>
<li>string tags have definitions to give them names</li>
<li>separate grammar and test files are supported</li>
<li>individual rules within a grammar can be tested</li>
<li>one test file can be called from within another</li>
<li>parser generation is via bytecode</li>
</ul>

<p>Other approaches to developing parsers are usually based on the <a
href="http://en.wikipedia.org/wiki/Context-free_grammar">context free
grammar</a> (CFG) formalism, the <a
href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">parsing
expression grammar</a> (PEG) notation, using <a
href="http://en.wikipedia.org/wiki/Parser_combinator">parser combinators</a> or
writing <a
href="https://en.wikipedia.org/wiki/Recursive_descent_parser">recursive
descent</a> parsers manually.</p>

<h3>CFG grammars</h3>

<p>The CFG notation (in the form of BNF and many variations) has been the main
formalism for grammars for a long time, and it has been well studied. However,
it has been argued quite strongly that it is not an ideal formalism
to use for unambiguous languages. According to <a
href="http://dl.acm.org/citation.cfm?id=964001.964011">Bryan Ford</a>:</p>

<blockquote>The power of generative grammars to express ambiguity is crucial to
their original purpose of modelling natural languages, but this very power makes
it unnecessarily difficult both to express and to parse machine-oriented
languages using CFGs.</blockquote>

<p>The most fundamental theoretical problem is that grammars can contain
ambiguities, and there is no computable algorithm to detect whether or not a
grammar is unambiguous. There is also the associated practical problem that many
published CFG grammars contain ambiguities. Some of these are local, i.e. a
particular rule allows two possible parses, but there is a surrounding more
global rule that resolves the ambiguity by only accepting one of the
possibilities. But many ambiguities in published grammars are inherent, the
ambiguity being resolved only by accompanying descriptive text or particular
approaches to generating parsers.</p>

<p>As well as ambiguity, there is also the problem that a CFG grammar describes
a language in a generative way, which is not directly and intuitively linked to
the recognition problem faced in parsing. Indeed the meaning of a grammar is
often different from the parser generated from it. There is an awkward semantic
gap.</p>

<p>Conventional CFG-based parser generators have a variety of other flaws,
partly due to the difficulties of the CFG formalism, and partly due to the age
of their designs. They often have arbitrary heuristic rules for disambiguation.
They often support a subset of CFG grammars which is not at all intuitive. They
often use bottom-up parsing techniques to make parsers near-linear, which makes
the way they operate impenetrable. And they often support actions and error
reporting using embedded code fragments, which is extremely fragile and
language-dependent approach.</p>

<p>These problems with the CFG formalism might be worth putting up with, if the
formalism had excellent theoretical or practical properties. But it doesn't.
For example, CFG grammars have poor composability properties, and poor closure
properties. And, in general, parsing CFG grammars takes O(n<sup>3</sup>) time in
the worst case.</p>

<h3>PEG grammars</h3>

<p>The main difference with PEG grammars is that the symmetrical choice operator
<code>|</code> is replaced by an ordered choice operator <code>/</code> which
has a more operational meaning. The expressive power of PEG grammars is
extremely close to that of CFG grammars. Examples such as palindromes which are
a problem for PEG grammars are typically also a problem for the efficient LR or
LALR subsets of CFG grammars.</p>

<p>The theoretical and practical properties of the PEG formalism are superior to
the CFG formalism in almost every measurable way. PEG grammars never contain
any ambiguity, they have a direct operational meaning as parsers, they are
composable, they are closed under various operations, and they can be parsed in
linear time using the packrat algorithm. This makes the PEG grammar formalism a
much better starting point for studying grammars and parsers.</p>

<p>Unfortunately, there are practical problems with PEG parsers. Although the
packrat algorithm is linear, it is slower and more space hungry than one would
like for efficient large scale parsing. Also, because if its relatively
uncontrolled backtracking, it is difficult to produce accurate error messages,
and difficult to express actions. And published PEG grammars can often be
rather unreadable, in the same way that regular expressions tend to be. As a
result, PEG grammars are usually only used for search expressions or for small
domain specific languages, and not for larger applications</p>

<p>There is also a relatively minor problem with PEG grammars, which is that
they don't support left recursion. There have been attempts to add it, but the
results are not compelling. It is worth noting, though, that left recursion is
arguably unintuitive, that it would probably never have become common if it
weren't for the prevalence of CFG grammars, and that it is well known how to
transform it away. Most uses of left recursion are very simple, of the kind
<code>a = x | a y</code> which can be translated easily into <code>a = x
y*</code>.</p>

<h3>Recursive Descent Parsing</h3>

<p>Recursive descent parsers have the advantage of being efficient, intuitive,
and capable of being hand-written. They can also be embedded in programming
languages, e.g. in the form of parser combinators.</p>

<p>Perhaps the main problem with recursive descent is that a simple approach is
hardly ever enough. In order to deal with the difficult cases which arise in
practice, there have to be some extra features such as lookahead. These extra
features are often added in an ad hoc fashion on practical grounds. That often
leads to a situation where it is difficult to tell exactly what range of
grammars or languages are supported by any given recursive descent system.</p>

<p>Embedding a parsing system into a language, e.g. in the form of combinators,
can be very convenient, especially for small domain specific languages where the
grammar is known in advance. A system like that has a natural feel for
programmers, often with good control over backtracking.</p>

<p>If, however, an approach is needed which is not tied to a particular
programming language, or if the grammar needs to be developed from scratch, or
translated from a different formalism (which is common because of the prevalence
of CFG grammars), or checked for consistency, or optimized to produce a fast
parser, then there is a case for a separate formalism such as Pecan.</p>

<h3>Pecan grammars</h3>

<p>Pecan uses a language which is based on the PEG notation. However, the
semantics is that of simple recursive descent, with explicitly controlled
lookahead and backtracking.</p>

<p>The language is independent of any host programming language. Nevertheless,
grammars can express actions and error reporting, and can be executed
symbolically for development and testing. Once developed, a grammar can be
converted into a parser in any desired language, either by hand or
automatically.</p>

<p>The Pecan system explicitly provides support for the process of developing
grammars, in the form of consistency checks on grammars, features for automated
testing, and a context in which manual transformations can be carried out.
Having a separate notation for grammars, rather than writing a parser by hand,
helps to focus attention on the various issues that have to be addressed,
separately from the programming details, but that also makes grammars rather
dense. If a grammar is not given in advance, creating it or translating it
accurately can be very difficult. In addition, programmers typically use parser
generators only rarely, and are often not grammar experts. As a result, support
for test-driven grammar development is an important feature of Pecan.</p>

<h2 id="install">Installation</h2>

<p>Pecan is a tool for developing grammars and generating parsers in a variety
of programming languages. The Pecan system is written in Java, so the first
step is to make sure a reasonably up-to-date version of Java is properly
installed. This can be tested by typing:</p>

<pre>javac -version
java -version
</pre>

<p>Pecan is provided as an executable jar file <code>pecan.jar</code>. The jar
file contains both the compiled program and also the source code. If
this jar file is downloaded, it can be run with:</p>

<pre>java -jar pecan.jar ...
</pre>

<p>However, it is more convenient to create a batch file or shell script, or use
an alias or equivalent, so that it can be run just by typing:</p>

<pre>pecan ...
</pre>

<p>The way to do this differs from system to system.</p>

<h2 id="notation">Notation</h2>

<p>The Pecan language is a grammar language for writing parsers, closely based
on the <a href="http://en.wikipedia.org/wiki/Parsing_expression_grammar">PEG</a>
notation. Pecan uses the sequential choice operator <code>x/y</code> but, unlike
the PEG notation, the operator does not automatically involve backtracking.
There are separate lookahead operators, for the controlled use of backtracking,
which provide greater control over the progress of parsing. It is much easier
to generate accurate error messages, and parsers can be kept reasonably
efficient by the programmer, without the space overhead of such techniques as
the PEG packrat algorithm.</p>

<p>As with PEG grammars, any parsing expression within a grammar represents a
matching operation which succeeds or fails when applied at a particular position
in the input, and which may cause progress, i.e. may cause the current position
to move forwards in the input, according to how much of it is matched. Unlike
PEG grammars, there is only a single possible outcome for this operation.</p>

<h3 id="rules">Rules</h3>

<p>A parser consists of a sequence of rules, each of which gives a name to a
parsing expression. By default, the first rule specifies the output of the
whole parsing process. For example, a parser might start with:</p>

<pre>module = function+ end
function = id arguments body
end = Uc!
...
</pre>

<p>Apart from being the starting point of the parser, the first rule is not
treated in any special way. In particular, it may succeed without consuming all
the input. So an explicit test is often included, as here, to check that the
end of the input has been reached. In the <code>end</code> rule,
<code>Uc!</code> means no Unicode character can be matched.</p>

<p>The text of a parser is assumed to be encoded using UTF-8, so Unicode
characters can be included. A rule name starts with any Unicode letter and
continues with Unicode letters or decimal digits or underscores or hyphens.</p>

<p>If a parser fails, only the first failure is reported. There is no attempt to
recover. If error recovery is required, this can be accomplished by writing a
separate rule to be tried on the remaining input.</p>

<h3 id="comments">Comments</h3>

<p>Comments start with <code>//</code> and extend to the end of the line. For
example:</p>

<pre>// A number is a sequence of one or more digits.
number = '0123456789'+
</pre>

<p>There is no multi-line comment convention.</p>

<h3 id="parentheses">Parentheses</h3>

<p>Parentheses, i.e. round brackets, have their usual meaning, indicating
grouping of operations. For example:</p>

<pre>(x / y) z
</pre>

<p>means parse <code>x</code> or <code>y</code>, then parse <code>z</code>, as
opposed to:</p>

<pre>x / y z
</pre>

<p>which means either parse <code>x</code>, or parse <code>y</code> and
<code>z</code>.</p>

<h3 id="continuations">Continuations</h3>

<p>No explicit symbol is used to terminate a rule. A rule is terminated at the
end of a line, unless it is continued. A rule is continued on the next line if
the last token on the current line is an infix symbol or open bracket, i.e. one
of <code>=/([</code>. Here is an example rule where each line except the last
ends with <code>=</code> or the <code>/</code> operator:</p>

<pre>atom =
  id /
  number /
  bracket
</pre>

<p>Also, a rule is continued on the next line if the first token of the next
line is an infix symbol or close bracket, i.e. one of <code>=/)]</code>. This
allows an alternative style for a multi-line rule:</p>

<pre>atom
= id
/ number
/ bracket
</pre>

<p>In this example, each line that starts with <code>=</code> or <code>/</code>
is a continuation of the previous line.</p>

<h3 id="choices">Choices</h3>

<p>The choice operator <code>/</code> separates alternatives, which are tried
one after the other. For example:</p>

<pre>atom = id / number / bracket
</pre>

<p>If parsing of an alternative succeeds, then the parsing of the whole
expression succeeds. Otherwise, if no progress was made, the next alternative is
tried.</p>

<p>The choice operator does not directly involve backtracking. If any progress
is made while trying an alternative, then the parser is committed to that
alternative, and further alternatives are not tried. Progress is made on an
alternative if at least one input character or token is matched.</p>

<p>There are separate lookahead operators which allow speculative parsing of an
alternative.</p>

<h3 id="sequences">Sequences</h3>

<p>When items follow each other with no visible operator in between, this
indicates "followed by". For example:</p>

<pre>assignment = identifier "=" expression
</pre>

<p>This means that an assignment is an identifier followed by an equals sign
followed by an expression. The items are parsed one by one and, if parsing of
any item fails, parsing of the sequence is abandoned. Sequencing binds tighter
than the choice operator.</p>

<h3 id="repetition">Repetition</h3>

<p>The three postfix repetition operators are <code>*</code> to indicate that
an item is to be repeated any number of times, i.e. zero or more
times, <code>+</code> to indicate that an item is to be repeated one or more
times, and <code>?</code>  to indicate that an item is optional (i.e. repeated
zero times or once). For example:</p>

<pre>string = '"' ('"'! visible)* '"'
number = digit+
call = function "(" arguments? ")"
</pre>

<p>The <code>string</code> rule specifies that a string consists of a double
quote followed by any number of visible characters followed by a closing double
quote. The <code>number</code> rule specifies that a number consists of one or
more digits. The <code>call</code> rule specifies that a call consists of a
function name followed by brackets, and the brackets may optionally contain
arguments.</p>

<p>The three expressions <code>x*</code>, <code>x+</code>, <code>x?</code> act
in exactly the same way as the three rules <code>xs</code>, <code>xp</code>,
<code>xq</code> defined by:</p>

<pre>xs = x xs / ""
xp = x xs
xq = x / ""
</pre>

<p>This means that <code>x*</code> or <code>x+</code> will accept as
many <code>x</code>'s as are present in the input. In all three cases, in line
with the fact that the choice operator does no backtracking by default, if
progress is made on a final <code>x</code> without success, the whole
expression fails. Postfix operators bind tighter than sequencing.</p>

<h3 id="lookahead">Lookahead</h3>

<p>There are three lookahead operators, the try operator <code>[...]</code>, the
has operator <code>&amp;</code> and the not operator <code>!</code>.</p>

<p>The try operator is specified using square brackets. In an expression
<code>[x]</code>, the subexpression <code>x</code> is parsed speculatively. If
it succeeds, parsing continues as normal. If it fails, the parser backtracks to
the point just before <code>x</code>. The try operator is usually used to
indicate the point at which the parser should commit to an alternative. For
example:</p>

<pre>statement = [identifier "="] expression / ...
</pre>

<p>This specifies that the parser should commit to the first alternative only
after matching the equal sign. If an error occurs before that, e.g. an
identifier has been matched but there is no equal sign, backtracking is done by
resetting the parser to the point before the identifier, and the next
alternative is tried.</p>

<p>The has <code>&amp;</code> and not <code>!</code> operators represent
positive and negative lookahead. In the PEG notation, <code>&amp;</code> and
<code>!</code> are prefix operators, but in Pecan, they are postfix, to make the
syntax of rules simpler and more uniform and, in particular, to avoid precedence
issues which would arise if there were both prefix and postfix operators.</p>

<p>With <code>x&amp;</code> or <code>x!</code>, the expression <code>x</code> is
parsed speculatively to check whether it appears next in the input or not.
Whether the expression <code>x</code> succeeds or fails, the parser backtracks
to the beginning. Then <code>x&amp;</code> succeeds if the parsing of
<code>x</code> succeeded, whereas <code>x!</code> succeeds if the parsing of
<code>x</code> failed. For example:</p>

<pre>statement = (type identifier)&amp; declaration / assignment
</pre>

<p>The parser looks ahead to see if there is a type and identifier next in the
input. If there is, the parser backtracks to the point before the type, but
continues with the same alternative and parses a declaration. If the lookahead
fails, an assignment is parsed. A negative example is:</p>

<pre>string = '"' ('"'! visible)* '"'
</pre>

<p>This says that a string contains any visible character other than a double
quote. The bracketed expression only tries to match a visible character if a
double quote does not appear next in the input.</p>

<p>The postfix <code>x&amp;</code> and <code>x!</code> operators bind more
tightly than sequencing.</p>

<p>During a try operation <code>[x]</code>, actions are delayed by storing them
symbolically. If <code>x</code> succeeds, the delayed actions are performed. If
<code>x</code> fails, the delayed actions are discarded. During
has and not operations <code>x&amp;</code> or <code>x!</code>, actions
are always discarded.</p>

<p>The fact that actions are not performed during speculative parsing restricts
the possible context sensitive aspects of a language, which can only depend on
actions outside of lookahead constructs.</p>

<h3 id="text">Text</h3>

<p>Text notations are used to match input characters. Double quotes indicate a
string of characters, which are matched in sequence. For example:</p>

<pre>keyword = "int" / "if" / "while" / ...
pi = "&#960;"
</pre>

<p>The string <code>"int"</code> only matches if all three characters appear in
the input in sequence, i.e. it is equivalent to <code>["i" "n" "t"]</code>. The
square brackets indicate that if a string matches only partially, the input
position returns to the beginning of the string, and other alternatives can be
tried. Any Unicode characters can be used in a string, using the UTF-8 encoding
in the grammar file.</p>

<p>Single quotes indicate a set of alternative characters, any one of which can
be matched. For example:</p>

<pre>op = '+-*/'
number = '0123456789'+
</pre>

<p>The expression <code>'+-*/'</code> matches any one of the four arithmetic
operator characters, and is equivalent to <code>'+' / '-' / '*' / '/'</code>.
The expression <code>'0123456789'</code> matches any digit.</p>

<p>Single characters can be represented using either single or double
quotes. For example:</p>

<pre>assign = id '=' exp
block = "{" statements "}"
double_quote = '"'
single_quote = "'"
</pre>

<p>Although the single quote set notation may contain Unicode characters, each
code point is treated as a separate character. In any situation where graphemes
might be involved, the string notation should be used instead. For example, the
set <code>'éè'</code> is visually ambiguous. It could contain two to four code
points, depending on whether combiners are used. Pecan does no normalization,
which must be handled via actions, so care needs to be taken. If combiners are
used yet the set needs to be regarded as containing only two characters, it
should be written:</p>

<pre>"é" / "è"
</pre>

<p>There is no escape convention within single or double quotes. To represent
control characters, or to represent Unicode characters using plain text,
integers are used. An integer on its own represents a single character, using
its decimal character code. For example:</p>

<pre>pi = 960
newline = 13? 10
</pre>

<p>If an integer is used which starts with a zero digit, then the character
code is in hexadecimal, using <code>a</code> to <code>f</code> or
<code>A</code> to <code>F</code>, e.g.</p>

<pre>pi = 03C0
newline = 0d? 0a
</pre>

<p>Common sets of characters can be specified using Unicode general categories.
The names <code>Uc, Cc, Cf, Cn, Co, Cs, Ll, Lm, Lo, Lt, Lu, Mc, Me, Mn, Nd, Nl,
No, Pc, Pd, Pe, Pf, Pi, Po, Ps, Sc, Sk, Sm, So, Zl, Zp, Zs</code> are provided.
The name <code>Uc</code> represents all Unicode characters
(<code>0..1114111</code>), and the others are the standard two-letter
abbreviations for the Unicode general categories which partition all the code
points. For example:</p>

<pre>letter = Lu / Ll / Lt / Lm / Lo
digit = Nd
connector = '_'! Pc
visible = (Cc/Cn/Co/Cs/Zl/Zp)! Uc
</pre>

<p>The Unicode categories starting with <code>L</code> are letters. The letter
rule allows any kind of letters: upper case, lower case, title case, modifier
and other. Categories starting with <code>N</code> are number characters, of
which <code>Nd</code> is the set of decimal digits. The connector rule uses the
<code>Pc</code> connector punctuation category, but excludes the underscore
character. The rule for <code>visible</code> excludes character code points
which are unassigned, private, surrogate, controls, or line or paragraph
separators.</p>

<p>A set of characters can be specified using a range. For
example:</p>

<pre>digit = '0'..'9'
letter = 'a'..'z' / 'A'..'Z'
visible = ' '..'~'
ascii = 0..127
</pre>

<p>Each argument to the range operator must be a single character, specified
using an integer or single quotes or a double quotes.</p>

<p>A further notation is a string of characters in angle brackets instead of
double quotes. An expression such as <code>&lt;abc></code> does not match any
input characters. It is a lookahead construct which succeeds if the remaining
input, regarded as a string, is lexicographically less than <code>"abc"</code>.
For example:</p>

<pre>keyword = &lt;f> keyword12 / keyword34
keyword12 = &lt;co> keyword1 / keyword2
keyword34 = &lt;sw> keyword3 / keyword4
keyword1 = "break" / "case" / "catch"
keyword2 = "continue" / "default" / "do" / "else"
keyword3 = "for" / "if" / "return" / "static"
keyword4 = "switch" / "try" / "while"
</pre>

<p>The <code>&lt;f></code> matches if the input, regarded as a
string, is lexicographically less than <code>"f"</code>.</p>

<pre>succeed = ""
fail = ''
</pre>

<p>They can occasionally be useful, e.g. in transformations.</p>

<h3 id="tags">Tags</h3>

<p>Tags support parsers where the input consists of tokens produced a separate
scanner. The input is thought of as an array of tokens instead of an array of
characters. Each tag represents a specific kind of token.</p>

<p>A tag is represented as a symbol consisting of the <code>%</code> character
followed by a name. For example, a token-based parser for a simple calculator
with no brackets might look like this:</p>

<pre>sum = term (%plus term @2add / %minus term @2subtract)* end
term = number (%times number @2multiply / %over number @2divide)*
number = %number @number
...
</pre>

<p>In this case, the input is an array of tokens with tags <code>%number</code>,
<code>%plus</code>, <code>%minus</code>, <code>%times</code>,
<code>%over</code>.</p>

<p>The tag <code>%</code> with no name is matched only at the end of the input.
This is equivalent to using <code>Uc!</code> in a scanner.</p>

<p>For greater readability, tags can be represented as strings. The simple
calculator could be rewritten as:</p>

<pre>sum = term ("+" term @2add / "-" term @2subtract)* end
term = number ("*" number @2multiply / "/" number @2divide)*
number = %number @number
...
</pre>

<p>Any string used must be given a definitions which relates it to a named
tag:</p>

<pre>"+" = %plus
"-" = %minus
"*" = %times
"/" = %over
</pre>

<p>This allows all tags to be associated with an enumerated type in the external
code when a parser is generated.</p>

<!--
Instead of the generated code being given an array of tokens, calls are made to
an external function which provides the next tag. This can be used to cope with
non-context-free features of languages. For example, take the infamous situation
with the C language where parsing depends on recognizing identifiers which have
been defined as type names using <code>typedef</code>. A C scanner which is
independent of parsing may emit tokens with an identifier tag, which may or may
not turn out later to be type names. The external functions for a C parser
would include symbol table construction, and could change the tags of tokens
given to the parser, according to those symbol tables.</p>
-->

<h3 id="actions">Actions</h3>

<p>Actions allow a parser to operate on values or data structures, using a
stack, to produce an output. An action is a symbol which consists of
the <code>@</code> character followed by a number followed by a name. If there
is no number, it is assumed to be zero. For example:</p>

<pre>sum = term (plus term @2add)*
</pre>

<p>Suppose that the <code>term</code> rule pushes a single item onto the stack,
and the <code>plus</code> rule doesn't affect the stack. The <code>@2add</code>
symbol in the <code>sum</code> rule indicates that two items should be popped
off the stack, the <code>add</code> action should be performed, which creates a
new item, and the new item should be pushed onto the stack. A parser as a whole
ends with a single item on the stack, which is the output from the parsing
process.</p>

<p>The stack is provided and manipulated by external code when a full parser is
generated. An action symbol such as <code>@2add</code> is translated into the
execution of a fragment of external code. The parser could be part of a
calculator, in which case the <code>add</code> action might add two numbers.
Alternatively, the parser might become part of a compiler, in which case the
<code>add</code> action might combine two expression trees into a larger tree.
When executing a grammar symbolically for testing, an action simply causes the
text of the action to be printed.</p>

<p>An action has access to the characters or tokens from the input which have
been matched since the previous action. For example:</p>

<pre>number = digit+ @number
</pre>

<p>The action <code>@number</code> creates a new item from the digits which
have just been matched, and pushes it onto the stack. There is a further
convention which allows matched characters to be discarded. For example:</p>

<pre>spaces = ' '+ @
</pre>

<p>An <code>@</code> sign on its own causes any recently matched characters to
be discarded by the parser, without any external code being executed.</p>

<p>There are severe consistency restrictions on actions. For example, each
alternative in a choice must create or consume the same number of output items.
Within a repetition, i.e. <code>x*</code> or <code>x+</code> or <code>x?</code>,
the inner expression <code>x</code> must have no net effect on the number of
output items. A rule must produce a fixed number of output items. The first
rule normally produced a single item, but this is not enforced, so that any
self-contained fragment of a grammar is legal, for testing.</p>

<p>The compensation for these severe restrictions is that Pecan can carry out
strong consistency checks on grammars, ensuring that every expression has the
right effect on the output stack, that the stack never underflows, and that a
fixed number of output items is produced overall. There is also the advantage
that actions are an integral part of the grammar, and are preserved during
transformations.</p>

<p>Scanners are normally thought of as producing a sequence of tokens. But in
Pecan, even a scanner must produce a fixed number of output items. A scanner
can be defined like this:</p>

<pre>tokens = @tokens token+ end
token = id @1id / number @1number / ...
</pre>

<p>The <code>@tokens</code> action creates an output item representing the list
or array of tokens to be generated, initially empty. Each action such as
<code>@1id</code> takes the recently matched characters, creates a token from
them, and adds it to the list.</p>

<p>In a token-based parser, there are often variable-length sequences, which
need to be dealt with in a similar way. For example, a comma-separated list of
identifiers can be expressed by:</p>

<pre>ids = @list id @2add ("," id @2add)* @1end
</pre>

<p>The <code>id</code> rule is assumed to push a single item onto the output
stack. The <code>@list</code> action creates an empty list item. The
<code>@2add</code> action pops the list and most recent id, adds the id to the
list, and pushes the result list back on the stack. The <code>@1end</code>
action does anything necessary to finalize the list.</p>

<p>The external code may choose a different implementation with the same overall
effect. For example, <code>@list</code> may mark a position in the output stack,
<code>@add</code> may do nothing so that ids are left on the stack, and
<code>@1end</code> may use the marked stack position to create an array out of
the accumulated ids.</p>

<p>Alternatively, <code>@list</code> may push two null pointers onto the output
stack, representing pointers to the first and last item of a linked list. The
first <code>@2add</code> may detect the two nulls and replace them by two
pointers to the first id. Each subsequent <code>@2add</code> may pop the last
item off the stack, chain the new id to it, and push the new item back on the
stack. Finally, the <code>@1end</code> action may discard the top item on the
stack, leaving the completed linked list.</p>

<p>Techniques like this can easily lead to a left hand alternative which begins
with an action, for example:</p>

<pre>@a x / y
</pre>

<p>The action <code>@a</code> is only performed if <code>x</code> succeeds,
or if it progresses before failing. If <code>x</code> fails without progressing
in the input, the action <code>@a</code> is not performed, and the next choice
<code>y</code> is tried. To avoid the need to undo actions, the action
<code>@a</code> is delayed. It is performed when <code>x</code> progresses, or
discarded if <code>x</code> fails without progressing.</p>

<h3 id="errors">Errors</h3>

<p>By default, a parser produces an error message which points to the furthest
position reached in the input text, other than in lookaheads, but which gives no
details. For example, suppose this rule is being parsed:</p>

<pre>sum = number ("+" number / "-" number)
</pre>

<p>Then a message like this might be produced for an incorrect operator:</p>

<pre>Error on line 1:
40~2
  ^
</pre>

<p>Markers can be added to parser rules, to describe the items which would have
allowed parsing to continue. A marker is a symbol consisting of
the <code>#</code> character followed by a name. For
example, suppose the rule above is changed to:</p>

<pre>sum = number (#plus "+" number / #minus "-" number)
</pre>

<p>Then, for an incorrect operator, the expressions <code>"+"</code>
and <code>"-"</code> both fail, so the error message produced becomes:</p>

<pre>Error on line 1: expecting minus, plus
40~2
  ^
</pre>

<p>When a marker is encountered, it is associated with the current position in
the input. It records something that the parser is expecting at that point. If
progress is made past that input position, the markers are cleared. When the
parser encounters an error, the set of things that the parser was expecting at
that point can be reported. Duplicates are removed, for example suppose the rule
is changed to:</p>

<pre>sum = number (#operator "+" number / #operator "-" number)
</pre>

<p>Then the error message becomes:</p>

<pre>Error on line 1: expecting operator
40~2
  ^
</pre>

<p>One way to ensure that all the items which could possibly allow parsing to
continue are reported is to write separate rules to describe low level features
of a grammar involving primitive character matchers, i.e. strings, sets,
character codes, or Unicode identifiers, and begin each with a marker. For
example, a parser for a programming language might contain rules such as:</p>

<pre>plus = #operator "+" " "* @
letter = #letter (Lu / Ll / Lt / Lm / Lo)
number = #number ('0'..'9')+ @number " "* @
newline = #newline 13? 10 @
end = #end Uc!
</pre>

<p>The <code>plus</code> rule specifies that the plus sign is to be described as
an operator in error messages, that it may be followed by optional spaces, and
that the plus sign and spaces are discarded. The <code>letter</code> rule
applies an error marker to a choice of Unicode categories, to avoid having to
attach a marker to each one individually.</p>

<p>With these rules, an error message like the one above might be generated,
saying that an operator is expected. The fact that an extra digit on the
preceding number, or a space, could also have allowed parsing to continue is not
reported. That is because individual digits and spaces are not marked in the
rules.</p>

<h2 id="testing">Testing</h2>

<p>During development of a grammar, testing can be carried out by symbolic
execution without having to generate a parser. This is done by typing a command
of the form:</p>

<pre>pecan [-trace] [line] testfile
</pre>

<p>The test file contains the tests to be carried out. The <code>-trace</code>
option switches on tracing, so that the individual steps taken during parsing
are displayed. If a line number is given, only the single test that starts on
that line of the test file is executed.</p>

<p>A test file contains a number of sections separated by lines consisting of at
least three equal signs. If a section contains a line of minus signs, then it is
a test with sample input and expected output. If a section has no divider, it is
a grammar which is used for subsequent tests, until another grammar is given.
For example:</p>

<pre>// Recognise one digit
number = ("0".."9") @number
==========
2
----------
number 2
==========
42
----------
number 4
==========
// Recognise any number of digits
number = ("0".."9") @number
==========
42
----------
number 42
</pre>

<p>The first section of this file sets up a grammar, then there are two tests
using that grammar, then a new grammar is given, then there is a final test
using the second grammar.</p>

<p>The output from running a test using a given grammar and sample input is a
list of the actions that would be performed by a parser generated from the
grammar. If the input is text rather than tokens, the characters matched since
the previous action or discard are displayed.</p>

<p>When a test fails, its line number is reported in the error message. That
line number can then be used to re-run the testing, but picking out only that
one test to be performed, perhaps with tracing:</p>

<pre>pecan tests.txt
Fail test on line 24 of tests.txt:
---------- Expected ----------
...
---------- Actual ----------
...

pecan -trace 24 tests.txt
</pre>

<p>A section of the test file can have some other special forms. Examples of
each possibility are:</p>

<pre>grammar.pecan
==========
tests.txt
==========
number
==========
// Comment
</pre>

<p>Each of these cases can be distinguished from a grammar, because there is no
rule which contains an equal sign. In the first example, a section consists of a
single line which is the name of a file containing a grammar to be used for
subsequent tests. This supports the common case where the grammar is in one file
and its tests are in another. The second example is the name of a secondary test
file to execute, allowing a suite of tests to be split across several files.
There is no conflict between the first two examples, because a grammar file is
equivalent to a test file which sets up a grammar but contains no tests. The
third example is the name of a rule in the current grammar, to be used as an
entry point for subsequent tests. It can be recognized by the lack of a dot
character. This allows a grammar to be developed rule by rule, with old tests on
particular rules being kept as regression tests. In the final example, a section
consists entirely of comment lines. The section has no effect, but can be used
to explain tests.</p>

<p>Test files are expected to use the UTF-8 encoding, but there is an escape
convention which allows control characters or Unicode characters to be included
as plain text. A backslash followed by digits represents a character by its
decimal code, or by its hex code if the code starts with zero. Two backslashes
are used to represent a single backslash, and a backslash followed by any other
character removes that character. In particular, a backslash followed by a space
can be used as a separator, and a backslash followed by a newline can be used to
cancel the newline. For example, given that <code>960</code> is the decimal code
for the character &#960;, then:</p>

<ul style="list-style-type:none;">
<li><code style="display:inline-block;width:5em;">\960x</code>
is &#960; followed by x</li>
<li><code style="display:inline-block;width:5em;">\960\ 5</code>
is &#960; followed by the digit <code>5</code></li>
<li><code style="display:inline-block;width:5em;">\\960x</code>
is the five characters <code>\960x</code></li>
<li><code style="display:inline-block;width:5em;">...\13\</code>
is a line ending in CR instead of LF</li>
</ul>

<h2 id="checks">Consistency Checks</h2>

<p>A number of checks are performed on a grammar which help to ensure
consistency. As well as obvious checks, such as syntax checking of the grammar,
and checking that each rule name is defined exactly once, the following further
checks are done:</p>

<h3 id="type">Type Checking</h3>

<p>A grammar is checked to see if it represents a text parser or a token parser.
A text parser must contain no tags. A token parser must not contain numerical
character codes or character sets or character ranges. If contains a string,
there must be a definition of that string as a synonym for a tag.</p>

<p>If the range operator is used, there is a check that each operand is a
character, i.e. a numerical character code or a one-character string or
one-character set, or a name which refers to one of those:</p>

<pre>digit = "0" .. "9"
letter = 'a' .. 'z' / 'A' .. 'Z'
control = NUL .. US
NUL = 0
US = 31
</pre>

<p>These checks are implemented by classifying every expression in the grammar
according to whether or not it refers to a single character.</p>

<h3 id="loop">Loop Checking</h3>

<p>Pecan checks that the grammar contains no obvious infinite loops. More
precisely, Pecan checks for left recursion. The simplest example is where a
rule mentions its own name at the start of its right hand side, or at the start
of one of its alternatives:</p>

<pre>sum = sum "+" term / term
</pre>

<p>In a Pecan grammar, a rule of this form leads to an immediate infinite loop,
and so is reported as an error.</p>

<p>Indirect left recursion is also detected. That is where two or more rules
mention each other at the beginning:</p>

<pre>expression1 = expression2 ...
expression2 = expression1 ...
</pre>

<p>Less obvious cases of left recursion are also detected, e.g.</p>

<pre>statement = label* statement
</pre>

<p>Although <code>statement</code> does not mention itself right at the
beginning, the expression <code>label*</code> may succeed without any input
being matched, and therefore an infinite loop ensues.</p>

<p>This check is implemented by finding out for each expression in the grammar
whether or not it is optional, i.e. whether it can succeed without making any
progress by matching some input. Then, for each rule, the rule names which it
starts with are recorded, taking account of initial optional expressions.
Finally, any loops within these recorded names are detected.</p>

<h3 id="token">Token Checking</h3>

<p>For grammars which have text as input, a check is made that all tokens
produced are non-empty, so that continual progress is made through the
characters in the input. For example:</p>

<pre>id = letter+ @identifier
letter = 'a' .. 'z'
</pre>

<p>Here, it is clear that by the time the accept action
<code>@identifier</code> is reached, at least one letter has been matched from
the input. On the other hand, suppose the <code>id</code> rule was:</p>

<pre>id = letter* @identifier
</pre>

<p>This causes an error message, because <code>@identifer</code> can be reached
without matching any input characters. However, if this <code>id</code> rule
is always used in a context where at least one character has already been
matched, i.e. the <code>id</code> rule refers to the remainder of a token
rather than a whole token, then there is no error.</p>

<p>This check is implemented by making as many deductions as possible about
positions in the grammar at which input characters have definitely been matched
since the previous token. This is a conservative check - it is possible for a
pathological grammar to produce an error message even though no empty token
would ever be created in practice.</p>

<p>There is a further check that a scanner makes no attempt to read past the
end of the input. For example, suppose there is a scanner rule like
this:</p>

<pre>tokens = token+ UC! @end
token =  space / identifier / keyword / operator / punctuation
</pre>

<p>This is fine, because all that follows the end of text <code>''</code> is an
action, not any attempt to match any characters. However, suppose the
rule is:</p>

<pre>tokens = token+
token = space / identifier / keyword / operator / punctuation / end
end = '' @end
</pre>

<p>This does cause an error, because after recognising the end of text, the
scanner could continue to look for more tokens.</p>

<h3 id="output">Output Checking</h3>

<p>Actions are assumed to treat output items in a stack-like manner. Restrictive
checks are made to guarantee consistent handling of output items. The arity of
each action has to be specified, and has to be consistent each time the action
appears. For example if a grammar contains both <code>@1add</code> and
<code>@2add</code>, that is reported as an error. The arities are used to check
that each expression in the grammar produces a fixed, known number of output
items.</p>

<p>Both alternatives in a choice expression must have the same net effect on the
size of the stack. For example, this rule causes an error:</p>

<pre>token = ('0'..'9')+ @token / ' '+
</pre>

<p>The first alternative adds one item to the stack, but the second adds
nothing. On the other hand, this rule is legal:</p>

<pre>token =  ('0'..'9')+ @1token / ' '+
</pre>

<p>The <code>@1token</code> action pops an item from the stack, presumably a
list of tokens, and pushes one item back on the stack, presumably the updated
list. It thus has zero net effect on the stack. Since both alternatives now have
a net zero effect, the <code>token</code> rule itself can be deduced as having a
net zero effect.</p>

<p>A repeated expression must have zero net effect on the stack. For example,
suppose the grammar contains:</p>

<pre>tokens = (('0'..'9')+ @token)*
</pre>

<p>The inner expression causes one output item to be pushed on the stack. This
is reported as an error, because <code>tokens</code> as a whole pushes an
unknown number of items onto the stack. On the other hand, this definition is
allowed:</p>

<pre>tokens = @list (('0'..'9')+ @1token)*
</pre>

<p>The <code>@list</code> action pushes one item, an empty list, onto the stack.
The inner expression pops the list, adds a token to it, and pushes the updated
list onto the stack. As a result, it has a zero net effect on the stack. It can
be repeated any number of times, still with a zero net effect, and so the
<code>tokens</code> rule can be deduced to push one item onto the stack.</p>

<p>A second check is that the stack never underflows. For example:</p>

<pre>example = "1" @n @2add
</pre>

<p>Here, when the <code>add</code>action is reached, there is only one output
item on the stack, whereas two previous output items are supposed to be passed
to <code>add</code>, so an error is reported.</p>

<p>To implement these checks, Pecan calculates the overall number of items added
to the stack, possibly negative, for each expression in the grammar. It also
calculates a low water mark value for every expression in the grammar,
representing the number of items that are needed on the stack during processing
of the expression. It then checks that for the first rule in the grammar, which
represents the final result, one item is added, and the low water mark isn't
negative.</p>

<p>In order to carry out these checks, a very uniform approach to actions has
to be taken. For example, Pecan checks that each alternative in a choice adds
the same number of items to the stack, and that where there is a repetition
operator, e.g. <code>x?</code> or <code>x*</code> or <code>x+</code>, the
subexpression <code>x</code> has no overall effect on the number of items on
the stack, so that the number of times <code>x</code> is repeated doesn't
have any overall effect on the stack size.</p>

<p>These checks are conservative, i.e. there could be pathological grammars
which always correctly produce one output item in practice, but which don't
pass the checks.</p>

<h2 id="java">Generating Code</h2>

<p>Code is generated as a bytecode with an interpreter. This is somewhat
similar to the table-driven techniques used in some other parser
generators. Advantages of using the bytecode approach are:</p>

<ul>

<li>it supports any target programming language</li>

<li>it avoids the repetition in hand-written parsers</li>

<li>it allows calls, which are very frequent, to be implemented in a simple
and efficient way using a custom stack</li>

<li>it makes various optimisations such as tail calls easier to implement</li>

</ul>

<h3>Interpreter templates</h3>

<p>An interpreter for the bytecode can be written in any language. It is written
as a template, which is filled in by the Pecan system from a grammar. The result
is a parser which matches the grammar.</p>

<p>The generated parser uses several sequences, which are generated by Pecan.
These are the opcodes, actions, errors, tags, rules, and the bytecode bytes.
Each of these can be inserted into the template using a placeholder in the
template such as:</p>

<pre>&lt;actions %s=%d, >
</pre>

<p>A placeholder starts with a less than sign <code>&lt;</code>, and one of the
keywords <code>opcodes</code>, <code>actions</code>, <code>errors</code>,
<code>tags</code>, <code>rules</code>, <code>bytes</code>. The keyword is
followed by a space, an insertion string and a greater than sign. The insertion
string works rather like the <code>printf</code> function in C and equivalent
formatted output functions in other languages. Each of the items in turn is
inserted into the template under control of the string. Where the string
contains the <code>%s</code> characters, the name of the item is printed. Where
the string contains <code>%d</code>, the decimal number for the item is printed.
Other characters are printed verbatim. The characters after the last percent
item are treated as separator characters, and are not printed for the last
item.</p>

<p>For example, suppose a text-based parser for a calculator contains actions
<code>@number</code>, <code>@2add</code>, <code>@2subtract</code>,
<code>@2multiply</code>, <code>@2divide</code>, and the template contains the
placeholder:</p>

<pre>&lt;actions %s=%d, >
</pre>

<p>Then the text inserted into the template at that point, to replace the
placeholder, is:</p>

<pre>number=0, add=1, divide=2, multiply=3, subtract=4
</pre>

<p>The actions are listed in order of arity and then, for each arity, in
alphabetical order. This helps to ensure that if actions in a scanner represent
token types, they can be matched up with the corresponding token tags in the
associated parser.</p>

<p>To create an enumerated type in C, the template could contain:</p>

<pre>enum action { &lt;actions %s, > };
</pre>

<p>The generated parser would then contain the line:</p>

<pre>enum action { number, add, divide, multiply, subtract };
</pre>

<p>Some example templates are provided.</p>

<h3>The nature of the bytecode</h3>

<p>A grammar is converted into an array of unsigned bytes. For example, suppose
this is the grammar:</p>

<pre>digit = "0".."9" @number
</pre>

<p>A template program is provided, containing a placeholder
<code>&lt;pecan></code> for the position of the bytecode sequence. Suppose the
template, in the C language, contains:</p>

<pre>...
unsigned char code[] = {
    &lt;pecan>
};
...
</pre>

<p>Once the bytecode sequence is inserted into the template, the result is:</p>

<pre>...
unsigned char code[] = {
    START, 8, GE, 1, 48, LE, 1, 57, ACT, number, STOP
};
...
</pre>

<p>The bytecode is generated symbolically by Pecan. The template must provide
definitions for the symbols in the bytecode, perhaps using enumerated types. The
symbols include opcodes, actions, error markers, tags, and possibly rules to be
used as entry points. The conversion from a grammar to a bytecode sequence uses
these translation rules, where commas between bytes are omitted:</p>

<pre>&lt;id = x>   =  RULE id START n<sub>x</sub> &lt;x> STOP
&lt;id>       =  GO n
&lt;'a'>      =  CHAR 'a'
&lt;10>       =  CHAR 10
&lt;128>      =  CHAR 194 128
&lt;"ab">     =  STRING 2 'a' 'b'
&lt;"&#960;">      =  STRING 2 207 128
&lt;'&#960;'>      =  STRING 2 207 128
&lt;'a&#960;'>     =  SET 3 'a' 207 128
&lt;'a'..'z'> =  RANGE 'a' 'z'
&lt;'&#945;'..'&#969;'> =  RANGE 206 177 207 137

0.."m"    RANGE...    LE n "m"
Nd        CAT Nd      CAT Nd
%id       TAG id      TAG n
#e        MARK e      MARK n


&lt;x / y>   =  EITHER n &lt;x> OR &lt;y>
&lt;x y>     =  BOTH n &lt;x> AND &lt;y>
&lt;x?>      =  MAYBE OPT &lt;x>
&lt;x*>      =  MAYBE MANY &lt;x>
&lt;x+>      =  DO THEN MAYBE MANY &lt;x>
&lt;[x]>     =  LOOK TRY &lt;x>
&lt;x&amp;>  =  LOOK HAS &lt;x>
&lt;x!>      =  LOOK NOT &lt;x>
&lt;@>       =  DROP
&lt;@a>      =  ACT0+n
</pre>

<p>The upper case words <code>RULE</code>, <code>START</code> etc. are one-byte
opcodes. An opcode may be followed by an argument. The notation
<code>&lt;x></code> stands for the sequence of bytes generated for expression
<code>x</code>, and <code>n<sub>x</sub></code> is its length.</p>

<p>In the sequence generated for a grammar rule, the <code>RULE</code>
instruction has no effect. It is only included to mark an entry point if
multiple entry points are requested, otherwise it is omitted. The argument to
the <code>RULE</code> opcode is the name of the rule, which the surrounding
program defines as an integer identifier. The program can scan the bytecode
sequence for entry points by using <code>n<sub>x</sub></code> to skip past the
<code>STOP</code> instruction, to find the next entry point.</p>

<p>The <code>START</code> instruction does any initialisation needed to begin a
new parsing operation.  Its argument is the length of the sequence
<code>&lt;x></code>. The length can also be thought of as a relative offset to
the <code>STOP</code> instruction. The <code>START</code> instruction then
pushes the address of the <code>STOP</code> instruction as a return address onto
an execution stack, then jumps to the start of the sequence <code>&lt;x></code>.
When <code>&lt;x></code> returns, it returns to the <code>STOP</code>
instruction, which finishes the parsing operation.</p>

<p>An expression which is just an identifier <code>id</code> is translated into
<code>GO n</code> where <code>n</code> is a relative offset to the sequence
<code>&lt;x></code>, where <code>x</code> is the right hand side of the rule
<code>id = x</code> which defines <code>id</code>.</p>

<p>In a straightforward bytecode implementation, with no optimization, each
expression is compiled into a bytecode function which succeeds or fails. As an
example, a choice <code>x / y</code> might be encoded as:</p>

<pre>&lt;x / y>  =  EITHER m &lt;x> OR n &lt;y> RETURN
</pre>

<p>Here <code>&lt;x></code> stands for the bytecode sequence generated from
subexpression <code>x</code>, and <code>m</code> stands for its length.
Similarly, <code>&lt;y></code> stands for the bytecode sequence generated from
subexpression <code>y</code>, and <code>n</code> stands for its length. The
<code>EITHER</code> opcode prepares for the choice, uses <code>m</code> to push
the address of the <code>OR</code> opcode onto the execution stack as a return
address, and jumps to the start of the <code>&lt;x></code> sequence. When that
sequence ends, it returns to the <code>OR</code> bytecode. This checks whether
it is appropriate to continue, pushes the address of the <code>RETURN</code>
bytecode onto the execution stack, and jumps to the start of the
<code>&lt;y></code> sequence. When that sequence ends, it returns to the
<code>RETURN</code> opcode, which returns from the <code>&lt;x / y></code>
sequence.</p>

<p>Immediately, there is an opportunity for a tail-call optimization. Instead of
calling <code>&lt;y></code>, the <code>OR</code> opcode can jump to
<code>&lt;y></code>, so the length <code>n</code> and the final
<code>RETURN</code> opcode are not needed. The sequence becomes:</p>

<pre>&lt;x / y>  =  EITHER m &lt;x> OR &lt;y>
</pre>

<p>Now, the <code>EITHER</code> opcode assumes that <code>X</code> immediately
follows <code>&amp;Y</code>. The full set of translations at this point,
without further optimisation, is:</p>

<pre>r = x    RULE frame         frame = amount of call stack needed
r        GO &amp;R              identifier: jump to its rule
x / y    EITHER OR &amp;Y       call Y if X fails without progress
x y      BOTH AND &amp;Y        call Y if X succeeds
x?       REPEAT ONCE        call X: fail without progress becomes success
x*       REPEAT MANY
x+       DO THEN REPEAT MANY
[x]      LOOK TRY
x&amp;       LOOK HAS
x!       LOOK NOT
#m       MARK M
%t       TAG T
'abc'    SET 3 a b c
"abc"    STRING 3 a b c
10       STRING 1 10
a..c     RANGE a c
Lu       CAT Lu
@        DROP
@f       ACT f
</pre>

<p>An important optimisation when generating code is to treat a choice between
alternatives as a switch, where possible. For example, suppose a grammar
contains a rule:</p>

<pre>token = id / number / string / ...
</pre>

<p>The alternative to be chosen depends on the next character in the input. An
alternative such as <code>number</code> starts with a character from a set such
as <code>'0123456789'</code>. Sometimes, the set may contain a single
character, e.g. the alternative <code>string</code> may start with
<code>'"'</code>. If these sets are disjoint, or can be made disjoint, then
a switch can be used to implement the choice.</p>

<p>Pecan takes a very uniform approach to this situation. First, the entire
grammar is analysed to find suitable disjoint sets of characters. Some of the
sets may contain just one character. For example, set <code>0</code> might
represent the upper case letters, set <code>1</code> the lower case letters,
set <code>2</code> might represent <code>'0123456789'</code>,
set <code>3</code> the character <code>'"'</code>, and so on. Each character
is classified into one of these sets. This is done on the whole input as a
separate pass before parsing begins, to avoid repetition as a result of
backtracking. The <code>token</code> rule can then be represented as a
switch:</p>

<pre>SWITCH &amp;id &amp;id &amp;number &amp;string ...
</pre>

<p>Both upper and lower case letters will trigger a jump to the <code>id</code>
rule, a digit will cause a jump to the <code>number</code> rule,
the <code>'"'</code> character causes a jump to the <code>string</code> rule,
and so on. This technique covers non-ascii characters and Unicode general
categories as well as simple ascii characters.</p>

<h3>The interpreter</h3>

<h2 id="side">Side Effects</h2>

<p>A grammar is linked with external code to make a practical parser. The
actions in the grammar become calls to this external code. As well as
manipulating the stack of output items, the external code may very well maintain
a state of some kind, and the actions may have side effects on that state.</p>

<p>Such side effects are awkward, because they interact badly with lookahead. On
the face of it, some system of undoing side effects would have to implemented,
so that backtracking could be guaranteed to have the right effect. As well as
returning to a previous position in the input, backtracking would need to undo
any effects caused by actions. In Pecan, at present, this isn't done. Instead,
actions are switched off during lookahead. It is assumed that lookahead is only
used to ask simple syntactic questions, in order to resolve ambiguities.</p>

<p>On the other hand, if side effects were not allowed to have any effect on
parsing, some features of some languages couldn't be handled. The most infamous
example is the C and C++ languages, where type names introduced using
<code>typedef</code> need to be categorized properly to parse programs, and yet
they can only be recognised after parsing their definitions, by maintaining a
symbol table during parsing. This is handled in Pecan by allowing side effects
to change the types of tokens which haven't yet been read.</p>

<h2 id="transforms">Transforms</h2>

<p>The Pecan grammar language is intended to be sufficiently simple and precise
that an equational theory of transforms can be developed. This theory could be
used to develop and justify automatic optimisations, or to build an assistant
which would suggest and check user-driven transforms while developing grammars.
The theory might include equations such as:</p>

<pre>(x y) z == x (y z)
(x / y) / z == x / (y / z)
x+ == x x*
x* = (x+)?
[[x] y] z == [x y] z
[x y] z / [x u] v == [x] ([y] z / [u] v)
(x / y) z = x z / y z
x y / x z == x (y / z)  <em>under suitable conditions</em>
x / y == y / x          <em>under suitable conditions</em>
</pre>

<p>This theory has yet to be established, but it is envisaged that it would
include the output building and error handling aspects of grammars.</p>

<h2 id="pecan">The Pecan Grammar</h2>

<p>Here is a Pecan parser for the Pecan language itself:</p>

<pre>pecan = skip rules Uc!
rules = rule (rules @2add)?
rule = id equals expression newline skip @2rule

expression = term (slash expression @2or)?
term = factor (term @2and)? / marker term @2handle
factor = look atom @2look / not atom @2not / atom repetition?
repetition = opt @2opt / any @2any / some @2some
atom = id / action / tag / range / back / bracket

id = letter alpha* @id gap
action = '@' (digit* letter alpha* @act / @drop) gap
tag = "%" letter alpha* @ask gap
marker = "#" letter alpha* @mark gap
range = text (".." skip @ text @2range)?
text = number / string / set
back = sb rule se @3back
bracket = rb rule re @3bracket

number = (("1".."9") digit* / "0" hex*) @number gap
string = '"' ('"'! visible)* '"' @string gap
set = "'" ("'"! visible)* "'" @set gap

equals = "=" infix
slash = "/" infix
look = "&amp;" prefix
not = "!" prefix
rb = '(' prefix
sb = '[' prefix
opt = "?" postfix
any = "*" postfix
some = "+" postfix
re = ')' postfix
sb = ']' postfix

infix = skip @
prefix = @token skip @
postfix = @token gap @
skip = (space / comment / newline)*
gap = space* comment? continuation @
continuation = [nl skip &amp;'=/)]']?
newline = (10 / 13 10?) @
comment = ["//"] visible* &amp;newline
visible = (Cc/Cn/Co/Cs/Zl/Zp)! Uc
alpha = letter / digit
letter = Lu / Ll / Lt / Lm / Lo
digit = Nd
hex = digit / 'ABCDEFabcdef'
</pre>
<!-- Write a Haskell version of Pecan, then prove it. -->

<!--
<h2>Notes</h2>

<p>Error sets are reported in a standard order (TreeSet) so that alternatives
can be swapped by the optimiser if they start differently, e.g. if there are
alternatives x/y/z and y starts with a keyword whereas x and z start with an
id, then the optimiser might transform to y/(x/z) to allow a case switch on the
outer choice, while the alternatives of the inner choice are kept in order.</p>

<h2>Implementation</h2>

<p>Generate direct code: problems are (b) incremental or
stream (c) no tail-call optn. Possible optimisations are: (a) inlining
especially for short non-recursive defs but also of initial calls to avoid
call-downs, and final calls to avoid tail-calls (although it isn't clear there
are many true tail-calls). (b) Case switching for alternatives. (c) @ID@
becomes SKIP. For case-switching in a parser, there can just be a case per
type (requires us to go back to constants!)  For case switching in a scan*,
there is a case per character, or a case per kind if compactness is needed,
where a kind is a minimal set.</p>

<p>Full table-driven approach: problems are (a) not much opportunity for
compiler optn (b) case switch per 'line of code' (c) not easy to
understand or debug</p>

<p>Want compromise. A case per rule doesn't work, because in</p>

<pre>case P:
  ok = CALL(Q);
  if (ok) ok = CALL(R);
</pre>

<p>the simulated call to Q, using our own stack, needs a return address between
the two statements, so the case labels are not enough.</p>

<p>A table where each position in the code represents an expression can
work:</p>

<pre>P:  AND P2 P3
P2: CALL Q
P3: CALL(R);
</pre>

<p>A problem is (again) what is the return address to put on the local stack
while calling Q? (It needs to be between the halves of the AND.)</p>

<p>Maybe a more-or-less direct translation of the direct code could work:</p>

<pre>P:  CALL Q   (represents ok = q() )
    IF P2         (represents if (ok) ok = r() )
    CALL(R)
P2: END           (represents if (!ok) out = out3; )
</pre>

<p>Case switch optimisation: Find all the starter sets for the alternatives,
and check overlap. If there are sufficient <i>distinct</i> sets, try for a
case switch, treating an error handler as "everything else". Sort, combining
alternatives with overlapping sets, making sure swapping is valid, and leaving
an error handler last. Aside from the error-handler, work out which new
alternative is best put last to act as default, to save cases. If all the
cases bar one lead to a small enough number of cases, use a character or
token-type switch. Otherwise, use a lookup to categorise the next character
and switch on sets.</p>

<h1>Old Pecan Notes</h1>

<p>Soft failure means input position doesn't change (though it may have been
reset by backtracking). Because of rule "commit only if input consumed", if
soft failure, output may have to be reset. In particular, apart from backtrack
or lookahead features, in a sequence, if it fails on second or subsequent item,
but without eating any input, then output position needs to be reset.</p>

<p>The <code>failWith</code> method usually returns the list of symbols which
have been tried but have failed, so that a standard message such as "expecting
name, number or string" can be generated. However, if the failure is due to an
error token in the input, that is taken to be a scanner error, and is reported
in preference to the expected symbol. If an error symbol is expected, then
that is reported in preference to any previous expected symbols, even ones
further on in the input.</p>

<h2>Scanner Implementation</h2>

<p>The set expressions are analysed. The smallest intersections are computed
and these form the base sets. Each set, possibly excluding any remaining
single characters, is expressed as a union of the base classes. A table is
computed where each character is looked up to find its base class. Preferably,
there are up to 8 or 16 or 32 or 64 bases, and the result can be a one-bit
number.</p>

<h2>Incremental Scanning</h2>

<p>Here are some restrictions so that a scanner can be used for incremental
scanning, as in a syntax highlighting editor.</p>

<ul>

<li>Each token is a non-empty exact substring of the source text, with a
classifying tag. The tokens exactly divide up the text, each starting where the
previous one ends. White space tokens can be discarded at some higher
level.</li>

<li>The scanner never fails. Error tokens are produced instead, distinguished
by tag. Scanning continues as normal afterwards. This allows syntax
highlighting of incorrect programs, and also allows scanning errors to be
reported during parsing so that, to the user, scanning and parsing appear to be
a single pass.</li>

</ul>

<p>With any rescanning mechanism, the amount of lookahead must be known:</p>

<ul>

<li>The amount of lookahead at the end of a token must be known. The lookahead
may be globally limited (e.g. always at most one character) or limited per
token type (two characters for a string literal). Otherwise, the scanner must
keep track of a "high water mark", i.e. the furthest point in the text that was
read from the start of scanning up to producing the current token, and use that
to form a global lookahead limit, or encode the lookahead in the token type.
Perhaps a max lookahead can be declared, and if the actual lookahead ever
exceeds that, an error or change of strategy is indicated. (The default can be
simply to track the max lookahead, and an option can switch on a lookahead per
token.)</li>

</ul>

<p>Re-scanning after a change of text consists of finding the rightmost
unaffected token, i.e. the end of the token plus the amount of lookahead does
not reach the text which has changed. Re-scanning continues until the text
resynchronises, i.e. a new token 'exactly matches' an old one.</p>

<p>Simple scanners can get away with a small fixed number of states. For
example, JSP mixes HTML with Java, or splitting multi-line (but not nested)
comments into one-line tokens to speed up incremental highlighting. For that,
you need:</p>

<ul>

<li>The scanner can be in one of a small number of 'memory free' states. Given
the current state and position in the source text, the next token can be found
purely by working forwards in the text. In other words, there must be no
backtracking back past the start of a token.</li>

<li>Each tag (token type) completely defines an end state, which is the start
state for scanning the next token. There is a fixed start state, and after
that the parser state is completely determined by the end state of the tag just
generated. For example, if scanning JSP, there can be a Java state and an HTML
state. The token tags distinguish between Java tokens, HTML tokens, and
transitional tokens in each direction. This would include distinguishing
between Java whitespace or error tokens, and HTML whitespace or error
tokens.</li>

</ul>

<p>Having a fixed number of parsing states is no good for nesting situations.
Nested comments require a state which counts the depth of nesting. Multiple
types of nested brackets require a state which keeps track of the whole
sequence of open brackets.</p>

<p>For interactive applications such as syntax highlighting, it is recommended
that very large tokens be avoided. For example, multi-line comments can be
divided into one-line pieces (with tags "start of comment", "middle of
comment", "end of comment"). This needs an extra scanner state ("inside
multi-line comment"). If the comments are nested, it needs an infinite number
of states. On the other hand, changing the start or end of a multiline comment
is likely to involve rescanning the whole token or the whole of the rest of the
source text anyway, and this is not necessarily slow, so maybe it is ok to
leave it.</p>

<p>An alternative is fully automated incremental scanning. An interpretive
approach is used with a custom execution stack. For each token, a position in
the execution stack is recorded. Although an individual rule never goes below
its original starting point, and indeed always returns exactly to it,
nevertheless further processing after the rule may combine the stack and destroy
the information before the rule's starting point. So, the stack needs to be
orgaised as a history stack. When a rule returns, instead of popping an item
off the stack, a skip is pushed on the stack saying that part of the stack is
to be ignored. See HistoryStack.java. The state at the time a token is
created can be stored with it, instead of assuming the type can be used as a
state.</p>

<h2>Stream Scanning</h2>

<p>Stream scanning can be done using the re-scanning mechanism. Scanning is
done a buffer of text at a time. Each new buffer is regarded as a change to
the end of the text, adding some more. For each buffer, all tokens where the
high water mark does not reach the end of the text can be emitted, before the
next buffer is processed.</p>

<h2>Incremental Scanning with Finite States</h2>

<p>It should be possible to check a normal grammer to see if it can be made
incremental with a fixed number of states rather than the more general history
stack mechanism. Subexpressions of the grammar (rules) are classified as (a)
not producing any tokens or (b) producing one token at the end or (c) looping.
A looping rule cannot appear as a subrule of a larger expression, except in
alternatives of a defined rule. A looping rule or alternative must consist of
a part which produces a token at the end, followed by the name of a defined
looping rule. Each occurrence of a particular token type must be followed by
the same looping rule. This should be enough to ensure that when a token is
produced, there is ony one tail-recursive return address on the execution
stack, and that is for a named rule which can be returned to later using just
the type of the previous token.</p>

<h2>Keywords</h2>

<p>Suppose a language has <code>int</code> as a keyword, but also allows
identifiers such as <code>intx</code> that begin with <code>int</code>. Then
there appear to be two possibilities. One is to use lookahead, the advantage
of which is that the rule for each keyword and the rule for identifiers can be
expressed separately:</p>

<pre>    token = int / ... / id
    int = "int" [non-letter-or-digit] ! #
    id = letter letter-or-digit* id#
</pre>

<p>Note, however, that the id rule must be the last alternative, to express the
fact that it excludes keywords, which does not fit in with the idea in PEG of
combining scanner and parser grammars, because then the id rule would have to
stand alone. The other alternative, avoiding lookahead, is for the identifier
rule to be intertwined with the keyword rules, e.g.:</p>

<pre>    token = "int" ! key / ... / other_id
    key = letter-or-digit+ id# / #
</pre>

<p>Either way, it would be helpful to rely on a couple of simple optimisations.
One is common prefix elimination, e.g.</p>

<pre>    x y / x z    == &gt;    x (y / z)
</pre>

<p>This helps to make sure that alternatives begin with disjoint character
sets. That allows a second optimisation, which is switching, e.g.:</p>

<pre>    'a' ... / 'b' ... / digit ... / ...
</pre>

<p>Instead of trying each alternative in turn, code is generated to (a) convert
each character into a small integer representing a disjoint character set, and
(b) do a switch on that integer to jump to the right alternative.</p>

<h2>Incremental Parsing</h2>

<p>To support this, a table-driven implementation is used, so that parsing can
be suspended, the current position in the execution stack can be changed, and
parsing can be resumed. The execution stack is a history stack.</p>

<p>After parsing, each node carries with it the start and end positions in the
source text, a lookahead based on the high water mark (furthest position
forward in the source text on creation) and the parsing end-state
(i.e. position in the execution stack) at the moment of creation.</p>

<p>The input is an array of characters or tokens with a current marker in it.
The output is an array (stack) of actions with an end marker. The actions
typically build nodes, but are not executed until parsing is complete, so that
backtracking can be done without side-effects. When each output action is
pushed, the position in the execution stack is recorded with it. After
parsing, the final execution stack is kept to support incremental
re-parsing.</p>

<p>To re-parse a region of source text which has changed, find the last output
action whose high water mark is before the changed text. Take the next node,
and move to the parent until the node's range covers the changed text. Use its
parsing start-state and start position to restart parsing. Stop reparsing when
synchronisation occurs (same end position and parsing state (and hwm?) when an
output action is created). The array of output actions consists of parts a, b
and c where b is the new part. The nodes corresponding in part a can be kept
intact. The actions in part b are used to create new nodes. Where nodes in
part c had nodes in part b as subnodes, those subnodes need to be replaced
somehow.</p>

<p>One way is to scan part c (but this doesn't feel good because it is not
limited). It is a rapid replay, simulating just the number of nodes beyond the
new ones, not their values. However, it almost certainly means replaying to
the end of the file, past a lot of irrelevant stuff.</p>

<p>Another is keep track of the correspondence between old and new nodes by
replay both b and oldb (the old counterpart of section b) together. Then
replace the old node in its parent by the new node. From each node, you need
to be able to reach its parent, and its position within its parent.</p>

<p>To make this work, a lot of consistency checks are needed. In particular,
if the execution stack ever goes below the point recorded in an output action,
this must be due to backtracking which will also remove the output action.</p>

<p>Each expression in the grammar forms a rule. A rule may consume input, may
produce output, and may succeed or fail. A failure is soft if no input has
been consumed, hard otherwise. A backtracking operation (lookahead or failed
commit) must restore the input position, and output position, and execution
stack, to what it was. A soft failure which consumes no input but produces
output must discard the output produced by restoring the output position to
what it was. For example, suppose there is a sequence <code>x y</code> where
<code>x</code> is just an output action, and then <code>y</code> fails. This
is a soft failure of <code>x y</code>, and any output produced by
<code>x</code> must be discarded.</p>

<h2>Interpretive Execution</h2>

<p>The purpose of custom interpretive execution is to have our own execution
stack, organised as a history stack which can be restarted at any point. It
contains return addresses and saved local variables. One choice for an
interpretive style of code seems to be this.</p>

<pre>SEQ x AND y AND z END

SEQ: push(out), call(p+1)   [[ = push(p+2), jump(*(p+1)) ]]
AND: if fail { out=pop(); return } else call(p+1)
call: if return address goes to END, TAILCALL

ALT: push(in), call(p+1)
OR: if succeed return; if (in > saveIn) return; call(p+1)
</pre>

<p>The AND instruction forms a return address for execution to return to after
executing the previous instruction. The call() operation checks for the end of
an AND sequence, and does a tail call instead. (An alternative would be to
have a different final version of AND, and/or to pre-process to change the last
ANDs to tail call versions.)</p>

<p>Alt: on success or hard failure of an alternative, return. On soft failure,
try next. Need to store 'in' to test for soft failure. Don't need to store
'out' because each alternative which soft-fails restores 'out', as does the one
which hard-fails.</p>

<p>Seq: on failure, restore 'out' and return. If all items succeed, return
success. Need to store 'out' to restore it. Restoring 'out' on hard failure
should be no problem, because parsing will crash or there is a surrounding
backtrack.</p>

<p>Try: save in, do x, if it fails restore in. Don't need to restore out if x
restores out on failure.</p>

<p>Look: save in and out, do x, if it fails restore in. If x succeeds, must
restore both in and out.</p>

<p>Opt: x? is essentially x / nothing. If x succeeds, ok. If x hard-fails,
ok. If x soft-fails, report success (and out has been restored). Must store
in to test for soft-fail.</p>

<p>Any: x* is x x* / nothing. If x succeeds n times and then hard fails, ok.
If x succeeds n times and then soft fails, out is restored by the soft failure,
report success. Need to store in to test for soft failure.</p>

<p>Some: x+ is x x*. If x fails, ok. If it succeeds, do as for Any. Need to
store in.</p>

<p>Atom: If succeeds, moves in and adds SKIP to output. Else soft-fails.
Cooperates in collecting expected bitset via global failPosition and
failTags. Not needed for re-parsing?</p>

<p>Act: add given action to output.</p>

<p>IDEA: can we check for separability of the scanner from the parser?  Maybe
like this. Take the whole grammar, and discard all the tree-like actions which
build nodes. Radically simplify the resulting grammar down to its bare
essentials. See if there is ever any backtracking past the creation of a token
which results in a different token or token type.</p>

<h2>Tutorial Topics</h2>

<p>Topics: grammar operators, left recursion, right recursion, iteration.
Stack-based actions. Actions not done and undone if backtrack. Actions can do
stuff or build trees. Built-in scanning and drawbacks. Separate scanner. How
to join then up. Incremental scanning. Incremental parsing?</p>
-->

</body>
</html>
